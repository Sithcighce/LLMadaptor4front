
import { useState, useCallback, useEffect, useMemo } from 'react';
import { TokenJS } from 'token.js/dist/index.cjs';
import { LlmClient } from '../client/LlmClient';
import type { ProviderId, ConnectorStatus, TokenUsage } from '../types/index';

const LOCAL_STORAGE_KEY = 'llm-connector-config';

/**
 * âš ï¸ INTERNAL: LLM Connector æ ¸å¿ƒåŸºç¡€è®¾æ–½ Hook
 * 
 * ðŸš¨ WARNING: æ­¤ Hook åˆ›å»ºç‹¬ç«‹çš„çŠ¶æ€å®žä¾‹ï¼Œä¸åº”ç›´æŽ¥åœ¨ç»„ä»¶ä¸­ä½¿ç”¨ï¼
 * 
 * æ­£ç¡®ç”¨æ³•ï¼š
 * âœ… ä½¿ç”¨ useLlmConnector() - é€šè¿‡ Context è®¿é—®å…±äº«çŠ¶æ€
 * âœ… ä½¿ç”¨ useConnectionManager() - å°è£…çš„è¿žæŽ¥ç®¡ç†æŽ¥å£
 * âŒ ç›´æŽ¥ä½¿ç”¨æ­¤ Hook - ä¼šåˆ›å»ºç‹¬ç«‹çŠ¶æ€å®žä¾‹ï¼Œå¯¼è‡´çŠ¶æ€ä¸åŒæ­¥
 * 
 * æ­¤ Hook çš„èŒè´£ï¼š
 * 1. ç®¡ç† llmClient å®žä¾‹ï¼ˆæœ€é‡è¦çš„æ ¸å¿ƒæš´éœ²ç‚¹ï¼‰
 * 2. å¤„ç†åŸºç¡€é…ç½®å’ŒçŠ¶æ€ç®¡ç†
 * 3. æä¾›æŒä¹…åŒ–å­˜å‚¨åŠŸèƒ½
 * 4. ä¸ºæ‰€æœ‰åŠŸèƒ½å±‚ Hook æä¾›åŸºç¡€æœåŠ¡
 * 
 * @internal ä»…ä¾› LlmConnectorProvider å†…éƒ¨ä½¿ç”¨
 * @see useLlmConnector æŽ¨èçš„å…¬å…±æŽ¥å£
 * @see useConnectionManager è¿žæŽ¥ç®¡ç†ä¸“ç”¨æŽ¥å£
 */
export const useLlmConnectorLogic = () => {
  // --- UI & Form State ---
  const [providerId, setProviderId] = useState<ProviderId>('openai');
  const [apiKey, setApiKey] = useState('');
  const [baseUrl, setBaseUrl] = useState('');
  const [model, setModel] = useState('gpt-4o'); // Default model

  // --- Connection State ---
  const [status, setStatus] = useState<ConnectorStatus>('disconnected');
  const [error, setError] = useState<Error | null>(null);
  const [llmClient, setLlmClient] = useState<LlmClient | null>(null);

  // --- Data State ---
  const [modelOptions, setModelOptions] = useState<string[]>([]);
  const [tokenUsage, setTokenUsage] = useState<TokenUsage | null>(null);

  // --- Load config from localStorage on initial mount ---
  useEffect(() => {
    try {
      const savedConfig = localStorage.getItem(LOCAL_STORAGE_KEY);
      if (savedConfig) {
        const { providerId, baseUrl, model } = JSON.parse(savedConfig);
        if (providerId) setProviderId(providerId);
        if (baseUrl) setBaseUrl(baseUrl);
        if (model) setModel(model);
      }
    } catch (e) {
      console.error('Failed to load config from localStorage', e);
    }
  }, []);

  // --- Save config to localStorage on change ---
  useEffect(() => {
    try {
      const configToSave = { providerId, baseUrl, model };
      localStorage.setItem(LOCAL_STORAGE_KEY, JSON.stringify(configToSave));
    } catch (e) {
      console.error('Failed to save config to localStorage', e);
    }
  }, [providerId, baseUrl, model]);

  // --- Fetch Models Handler (defined first to avoid circular dependency) ---
  const fetchModels = useCallback(async () => {
    if (!apiKey) {
      throw new Error('API Key is required to fetch models');
    }

    try {
      let models: string[] = [];

      if (providerId === 'openai') {
        const headers: Record<string, string> = {
          'Authorization': `Bearer ${apiKey.trim()}`,
        };
        
        const apiRoot = baseUrl || 'https://api.openai.com/v1';
        const response = await fetch(`${apiRoot}/models`, { headers });
        
        if (!response.ok) {
          const errorData = await response.json().catch(() => ({}));
          throw new Error(errorData.error?.message || `Invalid OpenAI API Key (${response.status})`);
        }
        
        const payload = await response.json();
        models = (payload.data ?? [])
          .filter((item: { id: string }) => item.id.includes('gpt')) // åªæ˜¾ç¤º GPT æ¨¡åž‹
          .map((item: { id: string }) => item.id)
          .sort();
          
        if (models.length === 0) {
          throw new Error('No GPT models found in your OpenAI account');
        }
        
      } else if (providerId === 'anthropic') {
        // å¯¹ Anthropic è¿›è¡Œç®€å•éªŒè¯ï¼šå‘é€ä¸€ä¸ªæœ€å°çš„æµ‹è¯•è¯·æ±‚
        const testHeaders = {
          'x-api-key': apiKey.trim(),
          'anthropic-version': '2023-06-01',
          'content-type': 'application/json'
        };
        
        const testResponse = await fetch('https://api.anthropic.com/v1/messages', {
          method: 'POST',
          headers: testHeaders,
          body: JSON.stringify({
            model: 'claude-3-haiku-20240307',
            max_tokens: 1,
            messages: [{ role: 'user', content: 'hi' }]
          })
        });
        
        if (!testResponse.ok) {
          const errorData = await testResponse.json().catch(() => ({}));
          throw new Error(errorData.error?.message || `Invalid Anthropic API Key (${testResponse.status})`);
        }
        
        // API Key æœ‰æ•ˆï¼Œè¿”å›žç¡¬ç¼–ç æ¨¡åž‹åˆ—è¡¨
        models = [
          'claude-3-5-sonnet-20241022',
          'claude-3-5-haiku-20241022', 
          'claude-3-opus-20240229',
          'claude-3-sonnet-20240229',
          'claude-3-haiku-20240307'
        ];
        
      } else if (providerId === 'gemini') {
        const apiRoot = baseUrl || 'https://generativelanguage.googleapis.com/v1beta';
        const url = `${apiRoot}/models?key=${encodeURIComponent(apiKey.trim())}`;
        
        const response = await fetch(url);
        
        if (!response.ok) {
          const errorData = await response.json().catch(() => ({}));
          throw new Error(errorData.error?.message || `Invalid Gemini API Key (${response.status})`);
        }
        
        const payload = await response.json();
        models = (payload.models ?? [])
          .filter((item: { name: string; supportedGenerationMethods?: string[] }) => 
            item.name.includes('gemini') && 
            item.supportedGenerationMethods?.includes('generateContent')
          )
          .map((item: { name: string }) => item.name.replace('models/', ''))
          .sort();
          
        if (models.length === 0) {
          throw new Error('No Gemini models found for your API Key');
        }
      } else {
        throw new Error(`Unsupported provider: ${providerId}`);
      }

      setModelOptions(models);
      
      console.log('fetchModels success:', { 
        providerId, 
        modelsCount: models.length, 
        models: models.slice(0, 3),
        currentModel: model 
      });
      
      return models;
      
    } catch (error) {
      console.error('Failed to fetch models:', error);
      setModelOptions([]);
      throw error; // é‡æ–°æŠ›å‡ºé”™è¯¯ç»™ handleConnect
    }
  }, [apiKey, providerId, baseUrl]); // ç§»é™¤ model å’Œ setModel ä¾èµ–ä»¥é¿å…å¾ªçŽ¯

  // --- é»˜è®¤æ¨¡åž‹è®¾ç½®é€»è¾‘ ---
  useEffect(() => {
    // å½“æ¨¡åž‹åˆ—è¡¨å˜åŒ–ä¸”å½“å‰æ¨¡åž‹ä¸åœ¨åˆ—è¡¨ä¸­æ—¶ï¼Œè®¾ç½®ç¬¬ä¸€ä¸ªæ¨¡åž‹ä¸ºé»˜è®¤
    if (modelOptions.length > 0 && !modelOptions.includes(model)) {
      setModel(modelOptions[0]);
    }
  }, [modelOptions, model, setModel]);

  // --- æ¨¡åž‹å˜åŒ–æ—¶é‡æ–°åˆ›å»º Client ---
  // ðŸš« æš‚æ—¶æ³¨é‡Šï¼šè¿™ä¸ª useEffect å¯¼è‡´æ— é™å¾ªçŽ¯ï¼Œå› ä¸º setLlmClient ä¼šè§¦å‘æ•´ä¸ªçŠ¶æ€æ ‘é‡å»º
  // useEffect(() => {
  //   // å½“æ¨¡åž‹å˜åŒ–ä¸”å·²è¿žæŽ¥æ—¶ï¼Œé‡æ–°åˆ›å»º client ä»¥ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„æ¨¡åž‹
  //   if (status === 'connected' && model && apiKey && providerId) {
  //     console.log('Model changed, recreating client:', { model, providerId });
  //     const config = {
  //       apiKey,
  //       baseURL: baseUrl || undefined,
  //     };
  //     const tokenJsClient = new TokenJS(config);
  //     const newClient = new LlmClient(tokenJsClient, providerId, model);
  //     setLlmClient(newClient);
  //   }
  // }, [model, status, apiKey, providerId, baseUrl]);

  // --- Handlers ---
  const handleConnect = useCallback(async () => {
    if (!apiKey) {
      setError(new Error('API Key is required.'));
      return;
    }
    setStatus('connecting');
    setError(null);

    try {
      // ç›´æŽ¥é€šè¿‡èŽ·å–æ¨¡åž‹æ¥éªŒè¯è¿žæŽ¥
      const fetchedModels = await fetchModels();
      
      // æ£€æŸ¥æ˜¯å¦èŽ·å–åˆ°æ¨¡åž‹
      if (fetchedModels.length === 0) {
        throw new Error('No models found for this provider');
      }
      
      // å¦‚æžœæ¨¡åž‹èŽ·å–æˆåŠŸï¼Œåˆ›å»º client å¹¶è®¾ç½®ä¸ºå·²è¿žæŽ¥
      const config = {
        apiKey,
        baseURL: baseUrl || undefined,
      };
      const tokenJsClient = new TokenJS(config);
      const client = new LlmClient(tokenJsClient, providerId, model);
      setLlmClient(client);
      setStatus('connected');
      
      console.log('handleConnect success:', { 
        status: 'connected', 
        providerId, 
        model,
        fetchedModelsCount: fetchedModels.length
      });
    } catch (e) {
      const connectError = e instanceof Error ? e : new Error('Connection failed: ' + String(e));
      setError(connectError);
      setStatus('disconnected');
      setLlmClient(null);
      setModelOptions([]); // æ¸…ç©ºæ¨¡åž‹åˆ—è¡¨
    }
  }, [apiKey, baseUrl, providerId, model, fetchModels]);

  const handleDisconnect = useCallback(() => {
    setLlmClient(null);
    setApiKey(''); // Clear API key on disconnect for security
    setStatus('disconnected');
    setError(null);
    setModelOptions([]); // Clear model list on disconnect
  }, []);

  // ðŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šç”¨ useMemo ç¨³å®šæ‰€æœ‰è¿”å›žå¯¹è±¡çš„å¼•ç”¨ï¼Œé¿å…æ— é™é‡æ¸²æŸ“
  const states = useMemo(() => ({
    providerId,
    apiKey,
    baseUrl,
    model,
    status,
    error,
    modelOptions,
    tokenUsage,
    llmClient,
  }), [providerId, apiKey, baseUrl, model, status, error, modelOptions, tokenUsage, llmClient]);

  // --- ä¸è‡ªåŠ¨èŽ·å–æ¨¡åž‹ï¼Œç”±ç”¨æˆ·æ‰‹åŠ¨è§¦å‘æˆ–åœ¨è¿žæŽ¥æ—¶èŽ·å– ---
  // useEffect(() => {
  //   if (apiKey && providerId) {
  //     fetchModels();
  //   }
  // }, [fetchModels]);

  const handlers = useMemo(() => ({
    setProviderId,
    setApiKey,
    setBaseUrl,
    setModel,
    handleConnect,
    handleDisconnect,
    fetchModels,
    setTokenUsage, // Will be used by the LlmClient wrapper later
  }), [setProviderId, setApiKey, setBaseUrl, setModel, handleConnect, handleDisconnect, fetchModels, setTokenUsage]);

  return useMemo(() => ({ 
    states, 
    handlers, 
    llmClient // ðŸŽ¯ æ ¸å¿ƒæš´éœ²ç‚¹ï¼šæ‰€æœ‰åŠŸèƒ½éƒ½åŸºäºŽè¿™ä¸ª client å®žä¾‹
  }), [states, handlers, llmClient]);
};
