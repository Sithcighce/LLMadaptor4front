
import { useState, useCallback, useEffect } from 'react';
import { TokenJS } from 'token.js/dist/index.cjs';
import { LlmClient } from '../client/LlmClient';
import type { ProviderId, ConnectorStatus, TokenUsage } from '../types';

const LOCAL_STORAGE_KEY = 'llm-connector-config';

/**
 * ðŸ”¥ LLM Connector æ ¸å¿ƒåŸºç¡€è®¾æ–½ Hook
 * 
 * è¿™æ˜¯æ•´ä¸ª LLM ç”Ÿæ€ç³»ç»Ÿçš„æ ¸å¿ƒåŸºç¡€è®¾æ–½ï¼Œè´Ÿè´£ï¼š
 * 1. ç®¡ç† llmClient å®žä¾‹ï¼ˆæœ€é‡è¦çš„æ ¸å¿ƒæš´éœ²ç‚¹ï¼‰
 * 2. å¤„ç†åŸºç¡€é…ç½®å’ŒçŠ¶æ€ç®¡ç†
 * 3. æä¾›æŒä¹…åŒ–å­˜å‚¨åŠŸèƒ½
 * 4. ä¸ºæ‰€æœ‰åŠŸèƒ½å±‚ Hook æä¾›åŸºç¡€æœåŠ¡
 * 
 * ðŸŽ¯ è®¾è®¡åŽŸåˆ™ï¼š
 * - ä¸“æ³¨äºŽæ ¸å¿ƒåŸºç¡€è®¾æ–½ï¼Œä¸åŒ…å«å…·ä½“çš„ UI ä¸šåŠ¡é€»è¾‘
 * - ä¸ºæ‰©å±•åŠŸèƒ½é¢„ç•™æŽ¥å£ï¼ˆllmClient æ˜¯æ ¸å¿ƒï¼‰
 * - å…¶ä»–åŠŸèƒ½ Hook éƒ½åº”è¯¥åŸºäºŽè¿™ä¸ªæ ¸å¿ƒæ¥æž„å»º
 * 
 * ðŸš€ æ‰©å±•è¯´æ˜Žï¼š
 * - æœªæ¥çš„èŠå¤©ã€å‚æ•°é…ç½®ã€å·¥å…·æ³¨å†Œç­‰åŠŸèƒ½éƒ½ä¼šåŸºäºŽ llmClient æ¥å®žçŽ°
 * - è¯·å‚è€ƒ hooks/README.md äº†è§£å®Œæ•´çš„æž¶æž„è®¾è®¡
 */
export const useLlmConnectorLogic = () => {
  // --- UI & Form State ---
  const [providerId, setProviderId] = useState<ProviderId>('openai');
  const [apiKey, setApiKey] = useState('');
  const [baseUrl, setBaseUrl] = useState('');
  const [model, setModel] = useState('gpt-4o'); // Default model

  // --- Connection State ---
  const [status, setStatus] = useState<ConnectorStatus>('disconnected');
  const [error, setError] = useState<Error | null>(null);
  const [llmClient, setLlmClient] = useState<LlmClient | null>(null);

  // --- Data State ---
  const [modelOptions, setModelOptions] = useState<string[]>([]);
  const [tokenUsage, setTokenUsage] = useState<TokenUsage | null>(null);

  // --- Load config from localStorage on initial mount ---
  useEffect(() => {
    try {
      const savedConfig = localStorage.getItem(LOCAL_STORAGE_KEY);
      if (savedConfig) {
        const { providerId, baseUrl, model } = JSON.parse(savedConfig);
        if (providerId) setProviderId(providerId);
        if (baseUrl) setBaseUrl(baseUrl);
        if (model) setModel(model);
      }
    } catch (e) {
      console.error('Failed to load config from localStorage', e);
    }
  }, []);

  // --- Save config to localStorage on change ---
  useEffect(() => {
    try {
      const configToSave = { providerId, baseUrl, model };
      localStorage.setItem(LOCAL_STORAGE_KEY, JSON.stringify(configToSave));
    } catch (e) {
      console.error('Failed to save config to localStorage', e);
    }
  }, [providerId, baseUrl, model]);

  // --- Handlers ---
  const handleConnect = useCallback(async () => {
    if (!apiKey) {
      setError(new Error('API Key is required.'));
      return;
    }
    setStatus('connecting');
    setError(null);

    try {
      const config = {
        apiKey,
        baseURL: baseUrl || undefined,
      };
      const tokenJsClient = new TokenJS(config);
      
      // For now, we can't fetch models from token.js, so we'll use a default
      // In the future, we could try a test call or use a method if token.js provides one.
      setModelOptions([model]);

      const client = new LlmClient(tokenJsClient, providerId, model);
      setLlmClient(client);
      setStatus('connected');
    } catch (e) {
      const connectError = e instanceof Error ? e : new Error('Failed to connect');
      setError(connectError);
      setStatus('disconnected');
      setLlmClient(null);
    }
  }, [apiKey, baseUrl, providerId, model]);

  const handleDisconnect = useCallback(() => {
    setLlmClient(null);
    setApiKey(''); // Clear API key on disconnect for security
    setStatus('disconnected');
    setError(null);
  }, []);

  const states = {
    providerId,
    apiKey,
    baseUrl,
    model,
    status,
    error,
    modelOptions,
    tokenUsage,
    llmClient,
  };

  // --- Fetch Models Handler ---
  const fetchModels = useCallback(async () => {
    if (!apiKey) {
      console.warn('API Key is required to fetch models');
      return;
    }

    try {
      let models: string[] = [];

      if (providerId === 'openai') {
        const headers: Record<string, string> = {
          'Authorization': `Bearer ${apiKey.trim()}`,
        };
        
        const apiRoot = baseUrl || 'https://api.openai.com/v1';
        const response = await fetch(`${apiRoot}/models`, { headers });
        
        if (!response.ok) {
          throw new Error(`Failed to fetch models (${response.status})`);
        }
        
        const payload = await response.json();
        models = (payload.data ?? []).map((item: { id: string }) => item.id);
        
      } else if (providerId === 'anthropic') {
        const headers: Record<string, string> = {
          'x-api-key': apiKey.trim(),
          'anthropic-version': '2023-06-01',
        };
        
        const apiRoot = (baseUrl || 'https://api.anthropic.com/v1/messages').replace(/\/messages?$/, '');
        const response = await fetch(`${apiRoot}/models`, { headers });
        
        if (!response.ok) {
          throw new Error(`Failed to fetch models (${response.status})`);
        }
        
        const payload = await response.json();
        models = (payload.data ?? []).map((item: { id?: string; name?: string }) => item.id ?? item.name).filter(Boolean) as string[];
        
      } else if (providerId === 'google') {
        const apiRoot = baseUrl || 'https://generativelanguage.googleapis.com/v1beta';
        const url = `${apiRoot}/models?key=${encodeURIComponent(apiKey.trim())}`;
        
        const response = await fetch(url);
        
        if (!response.ok) {
          throw new Error(`Failed to fetch models (${response.status})`);
        }
        
        const payload = await response.json();
        models = (payload.models ?? [])
          .filter((item: { name: string }) => item.name.includes('generateContent'))
          .map((item: { name: string }) => item.name.replace('models/', ''));
      }

      setModelOptions(models);
      
      // å¦‚æžœå½“å‰æ¨¡åž‹ä¸åœ¨åˆ—è¡¨ä¸­ï¼Œè®¾ç½®ç¬¬ä¸€ä¸ªæ¨¡åž‹ä¸ºé»˜è®¤
      if (models.length > 0 && !models.includes(model)) {
        setModel(models[0]);
      }
      
    } catch (error) {
      console.error('Failed to fetch models:', error);
      // å‘ç”Ÿé”™è¯¯æ—¶æ¸…ç©ºæ¨¡åž‹åˆ—è¡¨
      setModelOptions([]);
    }
  }, [apiKey, providerId, model, baseUrl]);

  // --- Fetch models when provider or apiKey changes ---
  useEffect(() => {
    if (apiKey && providerId) {
      fetchModels();
    }
  }, [fetchModels]);

  const handlers = {
    setProviderId,
    setApiKey,
    setBaseUrl,
    setModel,
    handleConnect,
    handleDisconnect,
    fetchModels,
    setTokenUsage, // Will be used by the LlmClient wrapper later
  };

  return { 
    states, 
    handlers, 
    llmClient // ðŸŽ¯ æ ¸å¿ƒæš´éœ²ç‚¹ï¼šæ‰€æœ‰åŠŸèƒ½éƒ½åŸºäºŽè¿™ä¸ª client å®žä¾‹
  };
};
