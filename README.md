# LLM Connector<p align="center">

  <img width="200px" src="https://raw.githubusercontent.com/Sithcighce/LLMadaptor4front/main/src/assets/logo.webp" />

<p align="center">  <h1 align="center">LLM Connector</h1>

  <img width="200px" src="https://raw.githubusercontent.com/Sithcighce/LLMadaptor4front/main/src/assets/logo.webp" /></p>

</p>

<p align="center">

ä¸€ä¸ª React åº“ï¼Œç”¨äºåœ¨å‰ç«¯åº”ç”¨ä¸­è§£å†³"è‡ªå¸¦ API Key"çš„é—®é¢˜ã€‚æ”¯æŒ 8 ä¸ª AI Providerï¼Œæä¾›ç»Ÿä¸€æ¥å£ã€‚  <a href="https://github.com/Sithcighce/LLMadaptor4front/actions/workflows/ci-cd-pipeline.yml"> <img src="https://github.com/Sithcighce/LLMadaptor4front/actions/workflows/ci-cd-pipeline.yml/badge.svg" /> </a>

  <a href="https://www.npmjs.com/package/llm-connector"> <img src="https://img.shields.io/npm/v/llm-connector?logo=semver&label=version&color=%2331c854" /> </a>

## ğŸš€ å¿«é€Ÿå¼€å§‹  <a href="https://www.npmjs.com/package/llm-connector"> <img src="https://img.shields.io/badge/react-16--19-orange?logo=react&label=react" /> </a>

</p>

```bash

npm install llm-connector## ç®€ä»‹

```

## ç®€ä»‹

**ğŸ¯ ç»Ÿä¸€çš„ LLM è¿æ¥å™¨åº“**: æ”¯æŒä¸‰ç§æ¨ç†æ¨¡å¼ - å‰ç«¯ BYOK (ç”¨æˆ·ç²˜è´´è‡ªå·±çš„ API Key)ã€ç«¯ä¾§æ¨ç† (Chrome AI/WebLLM æµè§ˆå™¨ç«¯æœ¬åœ°è¿è¡Œ)ã€åç«¯ä»£ç† (ä¼ä¸šç»Ÿä¸€ç®¡ç†/æœ¬åœ°æ¨¡å‹æœåŠ¡å™¨)ã€‚ä¸‰ç§æ¨¡å¼åŒç­‰é‡è¦,ç”¨æˆ·æ ¹æ®åœºæ™¯é€‰æ‹©ã€‚

`LLM Connector` is a React library designed to solve the "bring your own API key" problem in front-end applications. It provides a powerful logic core and a set of beautiful, independent UI components to let your users connect to hundreds of LLM providers securely and easily, with support for frontend BYOK, on-device inference, and backend proxy modes.

### åŸºç¡€ä½¿ç”¨

## The Problem

```tsx

import { LlmConnectorProvider, ConnectionFormEn, useLlmConnector } from 'llm-connector';In the world of AI applications, developers face a dilemma. End-users often have their own API keys from various sources:

- Official providers like OpenAI and Anthropic.

function App() {- Third-party proxy services that are cheaper or offer special models.

  return (- Self-hosted models with custom URLs.

    <LlmConnectorProvider>

      <ConnectionFormEn />Forcing users to re-configure these keys in every new app is tedious. Building a UI for this is repetitive for developers. Most existing solutions (like Vercel AI SDK) require a backend to manage keys, which doesn't fit the "user-provided key" model.

      <ChatComponent />

    </LlmConnectorProvider>## Our Solution

  );

}`LLM Connector` solves this by providing a **purely front-end** solution with two primary ways to use it:



const ChatComponent = () => {1.  **The Plug-and-Play Components**: A set of pre-built, independent React components for model selection, connection settings, and token usage display.

  const { llmClient } = useLlmConnector();2.  **The Logic Core**: A powerful set of React Hooks that handle all the state management, configuration persistence, and client creation, allowing you to build a completely custom UI.

  

  const sendMessage = async () => {At its heart, `LLM Connector` uses [Token.js](https://github.com/token-js/token.js) as its engine, enabling it to connect to over 200 LLM providers directly from the browser.

    const result = await llmClient.chat({

      messages: [{ role: 'user', content: 'Hello!' }]## Quick Start

    });

    console.log(result.text);Install the library:

  };```bash

  npm install llm-connector

  return <button onClick={sendMessage}>å‘é€æ¶ˆæ¯</button>;```

};

```### Option 1: The Plug-and-Play Way



## âœ¨ æ”¯æŒçš„ ProviderThis is the easiest way to get started. Wrap your application with `LlmConnectorProvider`, then place the independent UI components anywhere you like. They will work together automatically.



| Provider | ç±»å‹ | éœ€è¦ API Key |```tsx

|----------|------|--------------|import React from 'react';

| **Chrome AI** ğŸ†• | æµè§ˆå™¨å†…ç½® | âŒ |import {

| **LM Studio** ğŸ†• | æœ¬åœ°æœåŠ¡å™¨ | âŒ |  LlmConnectorProvider,

| **Silicon Flow** ğŸ†• | äº‘æœåŠ¡ï¼ˆä¸­å›½ï¼‰ | âœ… |  ConnectionFormEn,

| **Backend Proxy** ğŸ†• | åç«¯ä»£ç† | âœ… (åç«¯) |  ModelSelectEn,

| OpenAI | äº‘æœåŠ¡ | âœ… |  TokenUsageEn,

| Anthropic | äº‘æœåŠ¡ | âœ… |  useLlmConnector

| Google Gemini | äº‘æœåŠ¡ | âœ… |} from 'llm-connector';

| WebLLM | æµè§ˆå™¨ WASM | âŒ |

// A component to test the chat functionality

## ğŸ“– æ–‡æ¡£const ChatButton = () => {

  const { llmClient } = useLlmConnector();

- **å¼€å‘è€…äº¤æ¥**: [`docs/HANDOVER.md`](./docs/HANDOVER.md) â­

- **æ–°åŠŸèƒ½æŒ‡å—**: [`docs/NEW_PROVIDERS_GUIDE.md`](./docs/NEW_PROVIDERS_GUIDE.md)  if (!llmClient) return null;

- **æ¶æ„è®¾è®¡**: [`docs/AI-Adapter-Architecture.md`](./docs/AI-Adapter-Architecture.md)

- **å®Œæ•´æ–‡æ¡£**: [`docs/README.md`](./docs/README.md)  return (

- **åŸç‰ˆ README**: [`docs/README_ORIGINAL.md`](./docs/README_ORIGINAL.md)    <button onClick={async () => {

      const result = await llmClient.chat({ messages: [{ role: 'user', content: 'Hello!' }] });

## ğŸ› ï¸ å¼€å‘      alert(result.text);

    }}>

### ç¯å¢ƒè¦æ±‚      Say Hello

- Node.js 16+    </button>

- React 16-19  );

}

### å¯åŠ¨å¼€å‘æœåŠ¡å™¨

// Your main application

```bashfunction App() {

# å‰ç«¯æµ‹è¯•é¡µé¢  return (

npm run dev    <LlmConnectorProvider>

# è®¿é—® http://localhost:5173/      <header>

        <h2>My App</h2>

# åç«¯ä»£ç†æœåŠ¡å™¨ï¼ˆå¯é€‰ï¼‰        <ModelSelectEn />

cd backend-proxy      </header>

node server.js      <main>

# è¿è¡Œåœ¨ http://localhost:3003/        <p>Main content of my application...</p>

```        <ChatButton />

      </main>

### é¡¹ç›®ç»“æ„      <aside>

        <ConnectionFormEn />

```        <TokenUsageEn />

src/      </aside>

â”œâ”€â”€ providers/        # 8 ä¸ª Provider å®ç°    </LlmConnectorProvider>

â”œâ”€â”€ hooks/           # React Hooks (useLlmConnector ç­‰)  );

â”œâ”€â”€ components/      # UI ç»„ä»¶}

â”œâ”€â”€ test/           # æµ‹è¯•é¡µé¢```

â””â”€â”€ registry/       # å…¨å±€ Client æ³¨å†Œè¡¨

### Option 2: The Custom UI Way

backend-proxy/      # åç«¯ä»£ç†æœåŠ¡å™¨ï¼ˆExpress.jsï¼‰

docs/              # æ‰€æœ‰æ–‡æ¡£If you want to build your own UI from scratch, you can use the `useLlmConnector` hook, which provides all the states and handlers you need.

æœ€æ–°éœ€æ±‚/          # éœ€æ±‚æ–‡æ¡£

``````tsx

import React from 'react';

## âš ï¸ å½“å‰çŠ¶æ€import { LlmConnectorProvider, useLlmConnector } from 'llm-connector';



**ç‰ˆæœ¬**: v0.5.0-dev  const MyCustomConnectorUI = () => {

**çŠ¶æ€**: åŠŸèƒ½å¼€å‘å®Œæˆï¼Œæµ‹è¯•ä¸­  const { states, handlers, llmClient } = useLlmConnector();



**å·²çŸ¥é—®é¢˜**:  return (

- âš ï¸ æµ‹è¯•é¡µé¢æ˜¾ç¤ºé—®é¢˜å¾…ä¿®å¤    <div>

- âš ï¸ æ–° Provider åŠŸèƒ½æœªç»äººå·¥æµ‹è¯•      <p>Status: {states.status}</p>

      <input

è¯¦è§: [`docs/HANDOVER.md`](./docs/HANDOVER.md)        type="password"

        placeholder="Enter your API Key"

## ğŸ¤ è´¡çŒ®        value={states.apiKey}

        onChange={(e) => handlers.setApiKey(e.target.value)}

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼        disabled={states.status === 'connected'}

      />

å¼€å‘å‰è¯·é˜…è¯»:      <button onClick={handlers.handleConnect} disabled={states.status !== 'disconnected'}>

1. [`agentreadme.md`](./agentreadme.md) - Agent å¼€å‘æŒ‡å—        Connect

2. [`docs/HANDOVER.md`](./docs/HANDOVER.md) - é¡¹ç›®äº¤æ¥æ–‡æ¡£      </button>

      {llmClient && <p>Connected!</p>}

## ğŸ“„ License    </div>

  );

MIT}



---function App() {

  return (

**æ ¸å¿ƒç‰¹æ€§**:    <LlmConnectorProvider>

- âœ… çº¯å‰ç«¯è§£å†³æ–¹æ¡ˆ      <MyCustomConnectorUI />

- âœ… æ”¯æŒ 8 ä¸ª AI Provider    </LlmConnectorProvider>

- âœ… ç»Ÿä¸€æ¥å£ï¼ˆåŸºäº Token.jsï¼‰  );

- âœ… å¤šå®ä¾‹æ”¯æŒ}

- âœ… é…ç½®æŒä¹…åŒ–ï¼ˆlocalStorageï¼‰```

- âœ… TypeScript æ”¯æŒ

- âœ… React 16-19 å…¼å®¹## API Reference



**æ–°å¢åŠŸèƒ½** (v0.5.0):### Providers

- ğŸ†• Chrome AI - æµè§ˆå™¨å†…ç½® AI

- ğŸ†• LM Studio - æœ¬åœ°æ¨¡å‹æœåŠ¡å™¨-   **`<LlmConnectorProvider>`**: The root provider component that must wrap your application.

- ğŸ†• Silicon Flow - ä¸­å›½ LLM æä¾›å•†

- ğŸ†• Backend Proxy - åç«¯ä»£ç†æ¨¡å¼### Hooks


-   **`useLlmConnector()`**: The primary hook to access the connector's state and handlers from within the `LlmConnectorProvider`.
-   **`useLlmConnectorLogic()`**: The core logic hook. Use this only if you need to manage the logic outside of the provided context.

### UI Components

Each component comes in an English (`...En`) and Chinese (`...Zh`) version. They accept `className` and `locale` props for customization.

-   **`<ConnectionFormEn />` / `<ConnectionFormZh />`**: A form for setting the provider, API key, and base URL.
-   **`<ModelSelectEn />` / `<ModelSelectZh />`**: A dropdown for selecting the model.
-   **`<TokenUsageEn />` / `<TokenUsageZh />`**: A display for input and output token usage.

### Main Client

-   **`LlmClient`**: The standardized client object returned by the hook after a successful connection. Its main method is `chat(request)`.

## ğŸŒŸ Technical Innovation

### Explicit Client Naming System
LLM Connector introduces a breakthrough solution to React Context reliability issues:

```tsx
// Traditional Context mode - relies on "nearest parent"
const { llmClient } = useLlmConnector();

// Explicit naming mode - direct client specification
const { llmClient } = useLlmConnector('chat-client');
```

**Key Benefits:**
- âœ… **Deterministic binding** - No more "which Provider am I connected to?" confusion
- âœ… **Cross-component tree access** - Access named clients from anywhere
- âœ… **Configuration isolation** - Each instance has independent storage
- âœ… **Developer-friendly** - Clear error messages and debugging support

### Multi-Instance Support
Perfect for applications with different cost/performance requirements:

```tsx
// Chat with premium model
<LlmConnectorProvider name="chat" storageKey="chat-config">
  <ChatInterface />
</LlmConnectorProvider>

// Summary with economy model  
<LlmConnectorProvider name="summary" storageKey="summary-config">
  <SummaryInterface />
</LlmConnectorProvider>

// Access both from anywhere
function CrossComponentAccess() {
  const { llmClient: chatClient } = useLlmConnector('chat');
  const { llmClient: summaryClient } = useLlmConnector('summary');
}
```

## Known Issues

### UI Alignment with Demo
- **Issue**: Default UI components do not perfectly match the UIdemo.html styling and layout
- **Status**: âš ï¸ Pending resolution
- **Details**: The current React components should be aligned with the static UIdemo.html design for consistent visual appearance
- **Impact**: Visual inconsistencies between demo and actual component rendering

## Contributing

Contributions are welcome! Please open an issue or submit a pull request.

## License

This project is licensed under the MIT License.