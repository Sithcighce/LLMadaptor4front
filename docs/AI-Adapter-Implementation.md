# AI API é€‚é…å™¨ - è¯¦ç»†å®æ–½æŒ‡å—

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv1.0  
**åˆ›å»ºæ—¥æœŸ**ï¼š2025-10-20  
**é¢„è®¡å·¥æœŸ**ï¼š9ä¸ªå·¥ä½œæ—¥

---

## âš ï¸ é‡è¦æé†’ - ä¸‰ç§æ¨ç†æ¨¡å¼

**æœ¬é¡¹ç›®æ˜¯ç»Ÿä¸€çš„ LLM è¿æ¥å™¨åº“,æ”¯æŒä¸‰ç§åŒç­‰é‡è¦çš„æ¨ç†æ¨¡å¼:**

1. **å‰ç«¯ BYOK** - ç”¨æˆ·ç²˜è´´è‡ªå·±çš„ API Key,å‰ç«¯ç›´æ¥è°ƒç”¨
2. **ç«¯ä¾§æ¨ç†** - Chrome AIã€WebLLM æµè§ˆå™¨ç«¯æœ¬åœ°æ¨ç†
3. **åç«¯ä»£ç†** - Backend Proxyã€LM Studio åç«¯ç»Ÿä¸€ç®¡ç†

è¯¦è§: `docs/é¡¹ç›®å®šä½ä¸è¾¹ç•Œ.md`

---

## ğŸ“‹ å®æ–½é˜¶æ®µ

### Phase 1: æ–°å¢ Provider å®ç°ï¼ˆ3å¤©ï¼‰
### Phase 2: è°ƒç”¨æ¨¡å¼å±‚ï¼ˆ2å¤©ï¼‰
### Phase 3: åç«¯å®ç°ï¼ˆ2å¤©ï¼‰
### Phase 4: é›†æˆä¸æµ‹è¯•ï¼ˆ2å¤©ï¼‰

---

## ğŸ¯ Phase 1: æ–°å¢ Provider å®ç°ï¼ˆDay 1-3ï¼‰

### Day 1: Chrome AI Provider + LM Studio Provider

#### ä»»åŠ¡ 1.1ï¼šå®ç° ChromeAIProvider
**æ–‡ä»¶**: `src/providers/ChromeAIProvider.ts`

**å®Œæ•´ä»£ç **:
```typescript
import { LlmClient } from '../client/LlmClient';

// Chrome AI ç±»å‹å®šä¹‰
interface ChromeAICapabilities {
  available: 'readily' | 'after-download' | 'no';
  defaultTemperature?: number;
  defaultTopK?: number;
  maxTopK?: number;
}

interface ChromeAISession {
  prompt: (text: string) => Promise<string>;
  promptStreaming: (text: string) => AsyncIterable<string>;
  destroy: () => void;
  clone: () => Promise<ChromeAISession>;
}

declare global {
  interface Window {
    ai?: {
      assistant: {
        capabilities: () => Promise<ChromeAICapabilities>;
        create: (options?: {
          temperature?: number;
          topK?: number;
          systemPrompt?: string;
        }) => Promise<ChromeAISession>;
      };
    };
  }
}

/**
 * Chrome AI Provider
 * ä½¿ç”¨æµè§ˆå™¨å†…ç½®çš„ window.ai API
 * 
 * @see https://developer.chrome.com/docs/ai/built-in-apis
 */
export const createChromeAIProvider = async (): Promise<LlmClient> => {
  // 1. æ£€æŸ¥ Chrome AI å¯ç”¨æ€§
  if (!window.ai?.assistant) {
    throw new Error('Chrome AI is not available. Please use Chrome 127+ with AI features enabled.');
  }
  
  // 2. æ£€æŸ¥èƒ½åŠ›
  const capabilities = await window.ai.assistant.capabilities();
  
  if (capabilities.available === 'no') {
    throw new Error('Chrome AI is not supported on this device.');
  }
  
  if (capabilities.available === 'after-download') {
    throw new Error('Chrome AI is downloading. Please wait and try again.');
  }
  
  // 3. åˆ›å»ºä¼šè¯
  const session = await window.ai.assistant.create({
    temperature: capabilities.defaultTemperature || 0.7,
    topK: capabilities.defaultTopK || 3,
  });
  
  // 4. åˆ›å»ºå…¼å®¹ TokenJS æ¥å£çš„åŒ…è£…å™¨
  const chromeAIWrapper = {
    chat: {
      completions: {
        create: async (params: any) => {
          const messages = params.messages || [];
          const stream = params.stream || false;
          
          // Chrome AI åªæ”¯æŒç®€å•çš„ prompt
          // å°†å¤šè½®å¯¹è¯è½¬æ¢ä¸ºå•ä¸ª prompt
          const prompt = messages
            .map((msg: any) => `${msg.role}: ${msg.content}`)
            .join('\n\n');
          
          if (stream) {
            // æµå¼å“åº”
            const streamIterator = session.promptStreaming(prompt);
            let fullText = '';
            
            return {
              [Symbol.asyncIterator]: async function* () {
                for await (const chunk of streamIterator) {
                  fullText = chunk;
                  yield {
                    choices: [{
                      delta: { content: chunk },
                      index: 0,
                    }],
                  };
                }
              },
              // æ¨¡æ‹Ÿ TokenJS çš„æµå¼å“åº”æ ¼å¼
              async *[Symbol.asyncIterator]() {
                for await (const chunk of streamIterator) {
                  fullText = chunk;
                  yield {
                    choices: [{
                      delta: { content: chunk },
                      index: 0,
                    }],
                  };
                }
              },
            };
          } else {
            // éæµå¼å“åº”
            const response = await session.prompt(prompt);
            
            return {
              choices: [{
                message: {
                  content: response,
                  role: 'assistant',
                },
                finish_reason: 'stop',
                index: 0,
              }],
              usage: {
                prompt_tokens: 0,  // Chrome AI ä¸æä¾› token ç»Ÿè®¡
                completion_tokens: 0,
                total_tokens: 0,
              },
              model: 'chrome-ai',
              object: 'chat.completion',
            };
          }
        },
      },
    },
  };
  
  return new LlmClient(chromeAIWrapper, 'chrome-ai', 'chrome-ai-builtin');
};

/**
 * æ£€æŸ¥ Chrome AI æ˜¯å¦å¯ç”¨
 */
export const checkChromeAIAvailability = async (): Promise<{
  available: boolean;
  status: string;
}> => {
  if (!window.ai?.assistant) {
    return { available: false, status: 'not-supported' };
  }
  
  try {
    const capabilities = await window.ai.assistant.capabilities();
    return {
      available: capabilities.available === 'readily',
      status: capabilities.available,
    };
  } catch (error) {
    return { available: false, status: 'error' };
  }
};
```

#### ä»»åŠ¡ 1.2ï¼šå®ç° LMStudioProvider
**æ–‡ä»¶**: `src/providers/LMStudioProvider.ts`

**å®Œæ•´ä»£ç **:
```typescript
import { TokenJS } from 'token.js';
import { LlmClient } from '../client/LlmClient';

/**
 * LM Studio Provider
 * è¿æ¥æœ¬åœ°è¿è¡Œçš„ LM Studio æœåŠ¡å™¨
 * 
 * @see https://lmstudio.ai/docs/api
 */
export const createLMStudioProvider = async (
  baseUrl: string = 'http://localhost:1234/v1',
  model?: string
): Promise<LlmClient> => {
  // 1. æµ‹è¯•æœåŠ¡å™¨è¿æ¥
  try {
    const modelsResponse = await fetch(`${baseUrl}/models`, {
      headers: { 'Content-Type': 'application/json' },
      signal: AbortSignal.timeout(5000),  // 5ç§’è¶…æ—¶
    });
    
    if (!modelsResponse.ok) {
      throw new Error(`LM Studio server returned ${modelsResponse.status}`);
    }
    
    const modelsData = await modelsResponse.json();
    const models = modelsData.data || [];
    
    if (models.length === 0) {
      throw new Error('No models loaded in LM Studio. Please load a model first.');
    }
    
    // 2. é€‰æ‹©æ¨¡å‹
    const selectedModel = model || models[0].id;
    
    // éªŒè¯æ¨¡å‹æ˜¯å¦å­˜åœ¨
    const modelExists = models.some((m: any) => m.id === selectedModel);
    if (!modelExists && model) {
      console.warn(`Model "${model}" not found. Using "${models[0].id}" instead.`);
    }
    
    // 3. åˆ›å»º TokenJS å®ä¾‹
    // LM Studio å…¼å®¹ OpenAI API
    const tokenJs = new TokenJS({
      provider: 'openai',
      apiKey: 'lm-studio',  // LM Studio ä¸éœ€è¦çœŸå® API Key
      baseURL: baseUrl,
    });
    
    return new LlmClient(
      tokenJs,
      'lmstudio',
      modelExists && model ? model : models[0].id
    );
    
  } catch (error: any) {
    if (error.name === 'AbortError') {
      throw new Error('LM Studio connection timeout. Is the server running?');
    }
    throw new Error(`Failed to connect to LM Studio: ${error.message}`);
  }
};

/**
 * è·å– LM Studio å¯ç”¨æ¨¡å‹åˆ—è¡¨
 */
export const fetchLMStudioModels = async (
  baseUrl: string = 'http://localhost:1234/v1'
): Promise<string[]> => {
  try {
    const response = await fetch(`${baseUrl}/models`, {
      signal: AbortSignal.timeout(5000),
    });
    
    if (!response.ok) {
      throw new Error(`Server returned ${response.status}`);
    }
    
    const data = await response.json();
    return (data.data || []).map((m: any) => m.id);
    
  } catch (error: any) {
    throw new Error(`Failed to fetch models: ${error.message}`);
  }
};

/**
 * æ£€æŸ¥ LM Studio æœåŠ¡å™¨çŠ¶æ€
 */
export const checkLMStudioStatus = async (
  baseUrl: string = 'http://localhost:1234/v1'
): Promise<{
  online: boolean;
  modelsCount: number;
}> => {
  try {
    const models = await fetchLMStudioModels(baseUrl);
    return {
      online: true,
      modelsCount: models.length,
    };
  } catch (error) {
    return {
      online: false,
      modelsCount: 0,
    };
  }
};
```

#### ä»»åŠ¡ 1.3ï¼šå•å…ƒæµ‹è¯•
**æ–‡ä»¶**: `src/providers/__tests__/ChromeAIProvider.test.ts`

```typescript
import { createChromeAIProvider, checkChromeAIAvailability } from '../ChromeAIProvider';

describe('ChromeAIProvider', () => {
  beforeEach(() => {
    // Mock window.ai
    (global as any).window = {
      ai: {
        assistant: {
          capabilities: jest.fn().mockResolvedValue({
            available: 'readily',
            defaultTemperature: 0.8,
            defaultTopK: 3,
          }),
          create: jest.fn().mockResolvedValue({
            prompt: jest.fn().mockResolvedValue('Test response'),
            promptStreaming: jest.fn(),
            destroy: jest.fn(),
          }),
        },
      },
    };
  });
  
  it('should create provider successfully', async () => {
    const provider = await createChromeAIProvider();
    expect(provider).toBeDefined();
  });
  
  it('should check availability', async () => {
    const result = await checkChromeAIAvailability();
    expect(result.available).toBe(true);
    expect(result.status).toBe('readily');
  });
});
```

---

### Day 2: Silicon Flow Provider + Backend Proxy Provider

#### ä»»åŠ¡ 2.1ï¼šå®ç° SiliconFlowProvider
**æ–‡ä»¶**: `src/providers/SiliconFlowProvider.ts`

```typescript
import { TokenJS } from 'token.js';
import { LlmClient } from '../client/LlmClient';

/**
 * ç¡…åŸºæµåŠ¨ Provider
 * æ”¯æŒå¤šä¸ªå¼€æºæ¨¡å‹
 * 
 * @see https://docs.siliconflow.cn/
 */
export const createSiliconFlowProvider = (
  apiKey: string,
  model: string = 'Qwen/Qwen2.5-7B-Instruct'
): LlmClient => {
  if (!apiKey || apiKey.trim() === '') {
    throw new Error('Silicon Flow API key is required');
  }
  
  // ç¡…åŸºæµåŠ¨ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£
  const tokenJs = new TokenJS({
    provider: 'openai',
    apiKey: apiKey,
    baseURL: 'https://api.siliconflow.cn/v1',
  });
  
  return new LlmClient(tokenJs, 'siliconflow', model);
};

/**
 * ç¡…åŸºæµåŠ¨å¸¸ç”¨æ¨¡å‹
 */
export const SILICONFLOW_MODELS = [
  'Qwen/Qwen2.5-7B-Instruct',
  'Qwen/Qwen2.5-14B-Instruct',
  'Qwen/Qwen2.5-32B-Instruct',
  'Qwen/Qwen2.5-72B-Instruct',
  'THUDM/glm-4-9b-chat',
  'meta-llama/Meta-Llama-3.1-8B-Instruct',
  'meta-llama/Meta-Llama-3.1-70B-Instruct',
  'deepseek-ai/DeepSeek-V2.5',
] as const;

export type SiliconFlowModel = typeof SILICONFLOW_MODELS[number];
```

#### ä»»åŠ¡ 2.2ï¼šå®ç° BackendProxyProvider
**æ–‡ä»¶**: `src/providers/BackendProxyProvider.ts`

```typescript
import { LlmClient } from '../client/LlmClient';

/**
 * åç«¯ä»£ç† Provider
 * é€šè¿‡åç«¯ API è½¬å‘è¯·æ±‚ï¼ŒAPI Key å­˜å‚¨åœ¨æœåŠ¡å™¨
 */
export const createBackendProxyProvider = async (
  backendUrl: string = '/api/ai/proxy'
): Promise<LlmClient> => {
  // 1. å¥åº·æ£€æŸ¥
  try {
    const healthResponse = await fetch(`${backendUrl}/health`, {
      method: 'GET',
      signal: AbortSignal.timeout(5000),
    });
    
    if (!healthResponse.ok) {
      throw new Error(`Backend proxy returned ${healthResponse.status}`);
    }
    
    const healthData = await healthResponse.json();
    
    if (!healthData.ok) {
      throw new Error('Backend proxy is not healthy');
    }
    
  } catch (error: any) {
    throw new Error(`Backend proxy not available: ${error.message}`);
  }
  
  // 2. åˆ›å»ºåç«¯è°ƒç”¨åŒ…è£…å™¨
  const backendWrapper = {
    chat: {
      completions: {
        create: async (params: any) => {
          const { messages, model, stream, temperature, max_tokens } = params;
          
          const response = await fetch(`${backendUrl}/chat`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({
              messages,
              model: model || 'default',
              stream: stream || false,
              temperature,
              max_tokens,
            }),
          });
          
          if (!response.ok) {
            const errorData = await response.json().catch(() => ({}));
            throw new Error(errorData.message || `Backend error: ${response.status}`);
          }
          
          if (stream) {
            // æµå¼å“åº”
            const reader = response.body?.getReader();
            const decoder = new TextDecoder();
            
            return {
              async *[Symbol.asyncIterator]() {
                if (!reader) return;
                
                try {
                  while (true) {
                    const { done, value } = await reader.read();
                    if (done) break;
                    
                    const chunk = decoder.decode(value);
                    const lines = chunk.split('\n').filter(line => line.trim());
                    
                    for (const line of lines) {
                      if (line.startsWith('data: ')) {
                        const data = line.slice(6);
                        if (data === '[DONE]') continue;
                        
                        try {
                          const parsed = JSON.parse(data);
                          yield parsed;
                        } catch (e) {
                          console.error('Failed to parse SSE data:', data);
                        }
                      }
                    }
                  }
                } finally {
                  reader.releaseLock();
                }
              },
            };
          } else {
            // éæµå¼å“åº”
            return response.json();
          }
        },
      },
    },
  };
  
  return new LlmClient(backendWrapper, 'backend-proxy', 'backend-model');
};

/**
 * æ£€æŸ¥åç«¯ä»£ç†å¯ç”¨æ€§
 */
export const checkBackendProxyAvailability = async (
  backendUrl: string = '/api/ai/proxy'
): Promise<{
  available: boolean;
  models?: string[];
  error?: string;
}> => {
  try {
    const response = await fetch(`${backendUrl}/health`, {
      signal: AbortSignal.timeout(5000),
    });
    
    if (!response.ok) {
      return {
        available: false,
        error: `Server returned ${response.status}`,
      };
    }
    
    const data = await response.json();
    
    return {
      available: data.ok,
      models: data.models || [],
    };
    
  } catch (error: any) {
    return {
      available: false,
      error: error.message,
    };
  }
};
```

---

### Day 3: æ›´æ–° Provider Registry å’Œç±»å‹

#### ä»»åŠ¡ 3.1ï¼šæ›´æ–°ç±»å‹å®šä¹‰
**æ–‡ä»¶**: `src/types/index.ts`

```typescript
// === æ‰©å±• Provider ID ===
export type ProviderId = 
  | 'openai'
  | 'anthropic'
  | 'gemini'
  | 'webllm'
  | 'chrome-ai'      // ğŸ†•
  | 'lmstudio'       // ğŸ†•
  | 'siliconflow'    // ğŸ†•
  | 'backend-proxy'; // ğŸ†•

// === è°ƒç”¨æ¨¡å¼ ===
export type CallMode = 'frontend' | 'backend';

// === Provider é…ç½® ===
export interface ProviderConfig {
  id: ProviderId;
  name: string;
  description: string;
  requiresApiKey: boolean;
  requiresBaseUrl: boolean;
  supportsFrontend: boolean;
  supportsBackend: boolean;
  defaultModel?: string;
  modelOptions?: readonly string[];
}

// === è¿æ¥å™¨çŠ¶æ€ï¼ˆæ‰©å±•ï¼‰===
export interface ConnectorState {
  // åŸæœ‰çŠ¶æ€
  providerId: ProviderId;
  apiKey: string;
  baseUrl: string;
  model: string;
  status: ConnectorStatus;
  error: Error | null;
  modelOptions: string[];
  tokenUsage: TokenUsage | null;
  
  // æ–°å¢çŠ¶æ€
  callMode: CallMode;
  backendUrl: string;
  backendAvailable: boolean;
}

// === è¿æ¥å™¨å¤„ç†å™¨ï¼ˆæ‰©å±•ï¼‰===
export interface ConnectorHandlers {
  // åŸæœ‰å¤„ç†å™¨
  setProviderId: (id: ProviderId) => void;
  setApiKey: (key: string) => void;
  setBaseUrl: (url: string) => void;
  setModel: (model: string) => void;
  handleConnect: () => Promise<void>;
  handleDisconnect: () => void;
  fetchModels: () => Promise<void>;
  
  // æ–°å¢å¤„ç†å™¨
  setCallMode: (mode: CallMode) => void;
  setBackendUrl: (url: string) => void;
  checkBackendAvailability: () => Promise<boolean>;
}
```

#### ä»»åŠ¡ 3.2ï¼šåˆ›å»º Provider Registry
**æ–‡ä»¶**: `src/registry/providers.ts`

```typescript
import type { ProviderId, ProviderConfig } from '../types';
import { SILICONFLOW_MODELS } from '../providers/SiliconFlowProvider';

export const PROVIDER_REGISTRY: Record<ProviderId, ProviderConfig> = {
  openai: {
    id: 'openai',
    name: 'OpenAI',
    description: 'GPT-4, GPT-3.5 ç­‰æ¨¡å‹',
    requiresApiKey: true,
    requiresBaseUrl: false,
    supportsFrontend: true,
    supportsBackend: true,
    defaultModel: 'gpt-4o',
    modelOptions: ['gpt-4o', 'gpt-4o-mini', 'gpt-3.5-turbo'],
  },
  
  anthropic: {
    id: 'anthropic',
    name: 'Anthropic',
    description: 'Claude ç³»åˆ—æ¨¡å‹',
    requiresApiKey: true,
    requiresBaseUrl: false,
    supportsFrontend: true,
    supportsBackend: true,
    defaultModel: 'claude-3-5-sonnet-20241022',
    modelOptions: ['claude-3-5-sonnet-20241022', 'claude-3-opus-20240229'],
  },
  
  gemini: {
    id: 'gemini',
    name: 'Google Gemini',
    description: 'Gemini Pro, Flash ç­‰æ¨¡å‹',
    requiresApiKey: true,
    requiresBaseUrl: false,
    supportsFrontend: true,
    supportsBackend: true,
    defaultModel: 'gemini-pro',
    modelOptions: ['gemini-pro', 'gemini-pro-vision'],
  },
  
  webllm: {
    id: 'webllm',
    name: 'WebLLM',
    description: 'æµè§ˆå™¨å†…è¿è¡Œçš„ WASM æ¨¡å‹',
    requiresApiKey: false,
    requiresBaseUrl: false,
    supportsFrontend: true,
    supportsBackend: false,
    defaultModel: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',
  },
  
  'chrome-ai': {
    id: 'chrome-ai',
    name: 'Chrome AI',
    description: 'æµè§ˆå™¨å†…ç½® AIï¼ˆChrome 127+ï¼‰',
    requiresApiKey: false,
    requiresBaseUrl: false,
    supportsFrontend: true,
    supportsBackend: false,
    defaultModel: 'chrome-ai-builtin',
  },
  
  lmstudio: {
    id: 'lmstudio',
    name: 'LM Studio',
    description: 'æœ¬åœ°è¿è¡Œçš„æ¨¡å‹æœåŠ¡å™¨',
    requiresApiKey: false,
    requiresBaseUrl: true,
    supportsFrontend: true,
    supportsBackend: false,
    defaultModel: 'auto',
  },
  
  siliconflow: {
    id: 'siliconflow',
    name: 'ç¡…åŸºæµåŠ¨',
    description: 'å›½å†… API æœåŠ¡å•†',
    requiresApiKey: true,
    requiresBaseUrl: false,
    supportsFrontend: true,
    supportsBackend: true,
    defaultModel: 'Qwen/Qwen2.5-7B-Instruct',
    modelOptions: [...SILICONFLOW_MODELS],
  },
  
  'backend-proxy': {
    id: 'backend-proxy',
    name: 'Backend Proxy',
    description: 'é€šè¿‡åç«¯ä»£ç†è°ƒç”¨',
    requiresApiKey: false,
    requiresBaseUrl: true,
    supportsFrontend: false,
    supportsBackend: true,
    defaultModel: 'backend-default',
  },
};

/**
 * æ ¹æ®è°ƒç”¨æ¨¡å¼è¿‡æ»¤å¯ç”¨ Provider
 */
export const getAvailableProviders = (
  callMode: 'frontend' | 'backend'
): ProviderConfig[] => {
  return Object.values(PROVIDER_REGISTRY).filter(provider =>
    callMode === 'frontend' ? provider.supportsFrontend : provider.supportsBackend
  );
};
```

---

## ğŸ”„ Phase 2: è°ƒç”¨æ¨¡å¼å±‚ï¼ˆDay 4-5ï¼‰

### Day 4: æ‰©å±• useLlmConnectorLogic

#### ä»»åŠ¡ 4.1ï¼šæ·»åŠ æ–°çŠ¶æ€å’ŒæŒä¹…åŒ–
**æ–‡ä»¶**: `src/hooks/useLlmConnectorLogic.ts`

**ä¿®æ”¹éƒ¨åˆ†**:
```typescript
export const useLlmConnectorLogic = (storageKey: string = 'llm-connector-config') => {
  // === åŸæœ‰çŠ¶æ€ ===
  const [providerId, setProviderId] = useState<ProviderId>('openai');
  const [apiKey, setApiKey] = useState('');
  const [baseUrl, setBaseUrl] = useState('');
  const [model, setModel] = useState('gpt-4o');
  const [status, setStatus] = useState<ConnectorStatus>('disconnected');
  const [error, setError] = useState<Error | null>(null);
  const [modelOptions, setModelOptions] = useState<string[]>([]);
  const [tokenUsage, setTokenUsage] = useState<TokenUsage | null>(null);
  const [llmClient, setLlmClient] = useState<LlmClient | null>(null);
  
  // === æ–°å¢çŠ¶æ€ ===
  const [callMode, setCallMode] = useState<CallMode>('frontend');
  const [backendUrl, setBackendUrl] = useState('/api/ai/proxy');
  const [backendAvailable, setBackendAvailable] = useState(false);
  
  // === åŠ è½½é…ç½® ===
  useEffect(() => {
    try {
      const savedConfig = localStorage.getItem(storageKey);
      if (savedConfig) {
        const config = JSON.parse(savedConfig);
        
        if (config.providerId) setProviderId(config.providerId);
        if (config.baseUrl) setBaseUrl(config.baseUrl);
        if (config.model) setModel(config.model);
        if (config.callMode) setCallMode(config.callMode);         // ğŸ†•
        if (config.backendUrl) setBackendUrl(config.backendUrl);   // ğŸ†•
      }
    } catch (e) {
      console.error('Failed to load config:', e);
    }
  }, [storageKey]);
  
  // === ä¿å­˜é…ç½® ===
  useEffect(() => {
    try {
      const configToSave = {
        providerId,
        baseUrl,
        model,
        callMode,    // ğŸ†•
        backendUrl,  // ğŸ†•
      };
      localStorage.setItem(storageKey, JSON.stringify(configToSave));
    } catch (e) {
      console.error('Failed to save config:', e);
    }
  }, [providerId, baseUrl, model, callMode, backendUrl, storageKey]);
  
  // === æ£€æŸ¥åç«¯å¯ç”¨æ€§ ===
  const checkBackendAvailability = useCallback(async (): Promise<boolean> => {
    try {
      const result = await checkBackendProxyAvailability(backendUrl);
      setBackendAvailable(result.available);
      return result.available;
    } catch (e) {
      setBackendAvailable(false);
      return false;
    }
  }, [backendUrl]);
  
  // è¿”å›æ‰©å±•åçš„çŠ¶æ€å’Œå¤„ç†å™¨
  return {
    states: {
      // åŸæœ‰
      providerId,
      apiKey,
      baseUrl,
      model,
      status,
      error,
      modelOptions,
      tokenUsage,
      // æ–°å¢
      callMode,
      backendUrl,
      backendAvailable,
    },
    handlers: {
      // åŸæœ‰
      setProviderId,
      setApiKey,
      setBaseUrl,
      setModel,
      handleConnect,  // ç¨åä¿®æ”¹
      handleDisconnect,
      fetchModels,
      // æ–°å¢
      setCallMode,
      setBackendUrl,
      checkBackendAvailability,
    },
    llmClient,
  };
};
```

#### ä»»åŠ¡ 4.2ï¼šä¿®æ”¹è¿æ¥é€»è¾‘
**ç»§ç»­ä¿®æ”¹**: `src/hooks/useLlmConnectorLogic.ts`

```typescript
const handleConnect = useCallback(async () => {
  setStatus('connecting');
  setError(null);
  
  try {
    let client: LlmClient;
    
    // === æ ¹æ®è°ƒç”¨æ¨¡å¼é€‰æ‹© Provider ===
    if (callMode === 'backend') {
      // åç«¯ä»£ç†æ¨¡å¼
      client = await createBackendProxyProvider(backendUrl);
      
    } else {
      // å‰ç«¯ç›´è°ƒæ¨¡å¼
      switch (providerId) {
        case 'openai':
          if (!apiKey) throw new Error('OpenAI API key is required');
          client = createOpenaiProvider(apiKey, baseUrl);
          break;
          
        case 'anthropic':
          if (!apiKey) throw new Error('Anthropic API key is required');
          client = createAnthropicProvider(apiKey);
          break;
          
        case 'gemini':
          if (!apiKey) throw new Error('Gemini API key is required');
          client = createGeminiProvider(apiKey);
          break;
          
        case 'webllm':
          client = await createWebLlmProvider(model);
          break;
          
        case 'chrome-ai':  // ğŸ†•
          client = await createChromeAIProvider();
          break;
          
        case 'lmstudio':   // ğŸ†•
          if (!baseUrl) throw new Error('LM Studio base URL is required');
          client = await createLMStudioProvider(baseUrl, model);
          break;
          
        case 'siliconflow': // ğŸ†•
          if (!apiKey) throw new Error('Silicon Flow API key is required');
          client = createSiliconFlowProvider(apiKey, model);
          break;
          
        default:
          throw new Error(`Unknown provider: ${providerId}`);
      }
    }
    
    setLlmClient(client);
    setStatus('connected');
    
  } catch (err) {
    const error = err as Error;
    setError(error);
    setStatus('error');
    console.error('Connection failed:', error);
  }
}, [callMode, providerId, apiKey, baseUrl, model, backendUrl]);
```

---

### Day 5: æ›´æ–° UI ç»„ä»¶

#### ä»»åŠ¡ 5.1ï¼šæ‰©å±•é…ç½®é¢æ¿
**åˆ›å»ºæ–°æ–‡ä»¶**: `src/components/CallModeSelector.tsx`

```typescript
import React from 'react';
import type { CallMode } from '../types';

interface CallModeSelectorProps {
  callMode: CallMode;
  onChange: (mode: CallMode) => void;
  disabled?: boolean;
}

export const CallModeSelector: React.FC<CallModeSelectorProps> = ({
  callMode,
  onChange,
  disabled = false,
}) => {
  return (
    <div className="call-mode-selector">
      <label>è°ƒç”¨æ¨¡å¼</label>
      <div className="radio-group">
        <label>
          <input
            type="radio"
            value="frontend"
            checked={callMode === 'frontend'}
            onChange={(e) => onChange(e.target.value as CallMode)}
            disabled={disabled}
          />
          <span>å‰ç«¯ç›´è°ƒ</span>
          <small>ï¼ˆAPI Key åœ¨æµè§ˆå™¨ï¼‰</small>
        </label>
        
        <label>
          <input
            type="radio"
            value="backend"
            checked={callMode === 'backend'}
            onChange={(e) => onChange(e.target.value as CallMode)}
            disabled={disabled}
          />
          <span>åç«¯ä»£ç†</span>
          <small>ï¼ˆAPI Key åœ¨æœåŠ¡å™¨ï¼‰</small>
        </label>
      </div>
    </div>
  );
};
```

#### ä»»åŠ¡ 5.2ï¼šæ¡ä»¶æ˜¾ç¤ºé…ç½®é¡¹
**ä¿®æ”¹**: `src/components/ConfigPanel.tsx` (ç¤ºä¾‹)

```typescript
import { useLlmConnector } from '../hooks/useLlmConnector';
import { CallModeSelector } from './CallModeSelector';
import { PROVIDER_REGISTRY, getAvailableProviders } from '../registry/providers';

export const ConfigPanel: React.FC = () => {
  const { states, handlers } = useLlmConnector('my-chat');
  
  const { 
    callMode, 
    providerId, 
    apiKey, 
    baseUrl, 
    backendUrl,
    backendAvailable,
  } = states;
  
  const {
    setCallMode,
    setProviderId,
    setApiKey,
    setBaseUrl,
    setBackendUrl,
    checkBackendAvailability,
    handleConnect,
  } = handlers;
  
  const availableProviders = getAvailableProviders(callMode);
  const currentProvider = PROVIDER_REGISTRY[providerId];
  
  return (
    <div className="config-panel">
      {/* è°ƒç”¨æ¨¡å¼é€‰æ‹© */}
      <CallModeSelector
        callMode={callMode}
        onChange={setCallMode}
        disabled={states.status === 'connected'}
      />
      
      {/* Provider é€‰æ‹© */}
      {callMode === 'frontend' && (
        <div>
          <label>AI Provider</label>
          <select
            value={providerId}
            onChange={(e) => setProviderId(e.target.value as ProviderId)}
          >
            {availableProviders.map(provider => (
              <option key={provider.id} value={provider.id}>
                {provider.name} - {provider.description}
              </option>
            ))}
          </select>
        </div>
      )}
      
      {/* API Keyï¼ˆéœ€è¦çš„è¯ï¼‰*/}
      {callMode === 'frontend' && currentProvider.requiresApiKey && (
        <div>
          <label>API Key</label>
          <input
            type="password"
            value={apiKey}
            onChange={(e) => setApiKey(e.target.value)}
            placeholder="è¾“å…¥ API Key"
          />
        </div>
      )}
      
      {/* Base URLï¼ˆéœ€è¦çš„è¯ï¼‰*/}
      {callMode === 'frontend' && currentProvider.requiresBaseUrl && (
        <div>
          <label>Base URL</label>
          <input
            type="text"
            value={baseUrl}
            onChange={(e) => setBaseUrl(e.target.value)}
            placeholder="ä¾‹å¦‚: http://localhost:1234/v1"
          />
        </div>
      )}
      
      {/* åç«¯ URL */}
      {callMode === 'backend' && (
        <div>
          <label>Backend URL</label>
          <input
            type="text"
            value={backendUrl}
            onChange={(e) => setBackendUrl(e.target.value)}
            placeholder="/api/ai/proxy"
          />
          <button onClick={checkBackendAvailability}>
            æµ‹è¯•è¿æ¥
          </button>
          {backendAvailable && <span>âœ… åç«¯å¯ç”¨</span>}
        </div>
      )}
      
      {/* è¿æ¥æŒ‰é’® */}
      <button onClick={handleConnect}>
        {states.status === 'connecting' ? 'è¿æ¥ä¸­...' : 'è¿æ¥'}
      </button>
    </div>
  );
};
```

---

## ğŸ–¥ï¸ Phase 3: åç«¯å®ç°ï¼ˆDay 6-7ï¼‰

### Day 6: Next.js API Routes

#### ä»»åŠ¡ 6.1ï¼šå¥åº·æ£€æŸ¥æ¥å£
**æ–‡ä»¶**: `src/app/api/ai/proxy/health/route.ts`

```typescript
import { NextResponse } from 'next/server';

export async function GET() {
  // æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡
  const requiredEnvVars = [
    'OPENAI_API_KEY',
    // æˆ–å…¶ä»– Provider çš„ Key
  ];
  
  const missing = requiredEnvVars.filter(
    (key) => !process.env[key]
  );
  
  if (missing.length > 0) {
    return NextResponse.json({
      ok: false,
      error: `Missing environment variables: ${missing.join(', ')}`,
    }, { status: 500 });
  }
  
  return NextResponse.json({
    ok: true,
    models: ['gpt-4o', 'gpt-3.5-turbo'],  // å¯é…ç½®
    timestamp: new Date().toISOString(),
  });
}
```

#### ä»»åŠ¡ 6.2ï¼šèŠå¤©æ¥å£ï¼ˆéæµå¼ï¼‰
**æ–‡ä»¶**: `src/app/api/ai/proxy/chat/route.ts`

```typescript
import { NextRequest, NextResponse } from 'next/server';
import { TokenJS } from 'token.js';

// ç±»å‹å®šä¹‰
interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface ChatRequest {
  messages: ChatMessage[];
  model?: string;
  temperature?: number;
  max_tokens?: number;
  stream?: boolean;
}

export async function POST(request: NextRequest) {
  try {
    // 1. è§£æè¯·æ±‚
    const body: ChatRequest = await request.json();
    const { messages, model = 'gpt-4o', temperature, max_tokens, stream = false } = body;
    
    if (!messages || messages.length === 0) {
      return NextResponse.json({
        error: 'Messages are required',
      }, { status: 400 });
    }
    
    // 2. åˆ›å»º TokenJS å®ä¾‹ï¼ˆä½¿ç”¨æœåŠ¡å™¨ç¯å¢ƒå˜é‡ä¸­çš„ API Keyï¼‰
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey) {
      return NextResponse.json({
        error: 'Server configuration error',
      }, { status: 500 });
    }
    
    const tokenJs = new TokenJS({
      provider: 'openai',
      apiKey: apiKey,
    });
    
    // 3. è°ƒç”¨ LLM
    if (stream) {
      // æµå¼å“åº”
      const response = await tokenJs.chat.completions.create({
        provider: 'openai',
        model,
        messages,
        temperature,
        max_tokens,
        stream: true,
      });
      
      // åˆ›å»º SSE æµ
      const encoder = new TextEncoder();
      const customReadable = new ReadableStream({
        async start(controller) {
          try {
            for await (const chunk of response) {
              const data = JSON.stringify(chunk);
              controller.enqueue(
                encoder.encode(`data: ${data}\n\n`)
              );
            }
            controller.enqueue(encoder.encode('data: [DONE]\n\n'));
            controller.close();
          } catch (error) {
            controller.error(error);
          }
        },
      });
      
      return new NextResponse(customReadable, {
        headers: {
          'Content-Type': 'text/event-stream',
          'Cache-Control': 'no-cache',
          'Connection': 'keep-alive',
        },
      });
      
    } else {
      // éæµå¼å“åº”
      const response = await tokenJs.chat.completions.create({
        provider: 'openai',
        model,
        messages,
        temperature,
        max_tokens,
        stream: false,
      });
      
      return NextResponse.json({
        choices: response.choices,
        usage: response.usage,
        model: response.model,
      });
    }
    
  } catch (error: any) {
    console.error('Chat API error:', error);
    return NextResponse.json({
      error: error.message || 'Internal server error',
    }, { status: 500 });
  }
}
```

---

### Day 7: åç«¯é…ç½®å’Œç¯å¢ƒå˜é‡

#### ä»»åŠ¡ 7.1ï¼šåˆ›å»ºç¯å¢ƒå˜é‡æ¨¡æ¿
**æ–‡ä»¶**: `.env.example`

```bash
# OpenAI
OPENAI_API_KEY=sk-...

# Anthropic
ANTHROPIC_API_KEY=sk-ant-...

# Google Gemini
GEMINI_API_KEY=...

# ç¡…åŸºæµåŠ¨
SILICONFLOW_API_KEY=...

# é»˜è®¤ Providerï¼ˆå¯é€‰ï¼‰
DEFAULT_AI_PROVIDER=openai
DEFAULT_AI_MODEL=gpt-4o
```

#### ä»»åŠ¡ 7.2ï¼šå¤š Provider æ”¯æŒ
**ä¿®æ”¹**: `src/app/api/ai/proxy/chat/route.ts`

```typescript
// åœ¨ POST å‡½æ•°å¼€å¤´æ·»åŠ 
const providerId = process.env.DEFAULT_AI_PROVIDER || 'openai';

let tokenJs: TokenJS;

switch (providerId) {
  case 'openai':
    tokenJs = new TokenJS({
      provider: 'openai',
      apiKey: process.env.OPENAI_API_KEY!,
    });
    break;
    
  case 'anthropic':
    tokenJs = new TokenJS({
      provider: 'anthropic',
      apiKey: process.env.ANTHROPIC_API_KEY!,
    });
    break;
    
  case 'siliconflow':
    tokenJs = new TokenJS({
      provider: 'openai',
      apiKey: process.env.SILICONFLOW_API_KEY!,
      baseURL: 'https://api.siliconflow.cn/v1',
    });
    break;
    
  default:
    return NextResponse.json({
      error: `Unsupported provider: ${providerId}`,
    }, { status: 400 });
}
```

---

## ğŸ§ª Phase 4: é›†æˆä¸æµ‹è¯•ï¼ˆDay 8-9ï¼‰

### Day 8: é›†æˆåˆ° Google Audio

#### ä»»åŠ¡ 8.1ï¼šå®‰è£… llm-connector åŒ…
```bash
npm install llm-connector
# æˆ–
pnpm add llm-connector
```

#### ä»»åŠ¡ 8.2ï¼šä¿®æ”¹ useChat Hook
**æ–‡ä»¶**: `src/hooks/useChat.ts`

**åŸæœ‰å®ç°** (ç®€åŒ–):
```typescript
export const useChat = () => {
  const [messages, setMessages] = useState<Message[]>([]);
  
  const sendMessage = async (content: string) => {
    const response = await fetch('/api/chat', {
      method: 'POST',
      body: JSON.stringify({ messages: [...] }),
    });
    // ...
  };
  
  return { messages, sendMessage };
};
```

**æ–°å®ç°** (ä½¿ç”¨ LlmConnector):
```typescript
import { useLlmConnector } from 'llm-connector';

export const useChat = () => {
  const [messages, setMessages] = useState<Message[]>([]);
  const { llmClient, states } = useLlmConnector('google-audio-chat');
  
  const sendMessage = async (content: string) => {
    if (!llmClient) {
      throw new Error('LLM client not connected');
    }
    
    const newMessages = [
      ...messages,
      { role: 'user', content },
    ];
    
    setMessages(newMessages);
    
    try {
      const result = await llmClient.chat({
        messages: newMessages,
        stream: true,
      });
      
      let assistantMessage = '';
      
      for await (const chunk of result) {
        const delta = chunk.choices[0].delta.content || '';
        assistantMessage += delta;
        
        setMessages([
          ...newMessages,
          { role: 'assistant', content: assistantMessage },
        ]);
      }
      
    } catch (error) {
      console.error('Chat error:', error);
    }
  };
  
  return { messages, sendMessage, llmStatus: states.status };
};
```

#### ä»»åŠ¡ 8.3ï¼šæ·»åŠ  Provider
**æ–‡ä»¶**: `src/app/layout.tsx`

```typescript
import { LlmConnectorProvider } from 'llm-connector';

export default function RootLayout({ children }: { children: React.ReactNode }) {
  return (
    <html>
      <body>
        <LlmConnectorProvider
          name="google-audio-chat"
          storageKey="google-audio-llm-config"
        >
          {children}
        </LlmConnectorProvider>
      </body>
    </html>
  );
}
```

---

### Day 9: æµ‹è¯•å’Œæ–‡æ¡£

#### ä»»åŠ¡ 9.1ï¼šåŠŸèƒ½æµ‹è¯•æ¸…å•

**æµ‹è¯•ç”¨ä¾‹**:
```markdown
## Provider æµ‹è¯•

### Chrome AI
- [ ] æ£€æµ‹å¯ç”¨æ€§
- [ ] è¿æ¥æˆåŠŸ
- [ ] å•è½®å¯¹è¯
- [ ] å¤šè½®å¯¹è¯
- [ ] é”™è¯¯å¤„ç†

### LM Studio
- [ ] æœåŠ¡å™¨è¿æ¥
- [ ] æ¨¡å‹åˆ—è¡¨è·å–
- [ ] å¯¹è¯åŠŸèƒ½
- [ ] è¿æ¥è¶…æ—¶å¤„ç†

### Silicon Flow
- [ ] API Key éªŒè¯
- [ ] æ¨¡å‹é€‰æ‹©
- [ ] å¯¹è¯åŠŸèƒ½
- [ ] æµå¼å“åº”

### Backend Proxy
- [ ] å¥åº·æ£€æŸ¥
- [ ] éæµå¼å“åº”
- [ ] æµå¼å“åº”
- [ ] é”™è¯¯å¤„ç†

## è°ƒç”¨æ¨¡å¼æµ‹è¯•
- [ ] å‰ç«¯ç›´è°ƒ â†’ åç«¯ä»£ç†åˆ‡æ¢
- [ ] é…ç½®æŒä¹…åŒ–
- [ ] å¤šå®ä¾‹éš”ç¦»
```

#### ä»»åŠ¡ 9.2ï¼šåˆ›å»ºä½¿ç”¨æ–‡æ¡£
**æ–‡ä»¶**: `docs/AI apiå¼€å‘/ä½¿ç”¨æŒ‡å—.md`

```markdown
# AI API é€‚é…å™¨ä½¿ç”¨æŒ‡å—

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–
\`\`\`bash
npm install llm-connector
\`\`\`

### 2. æ·»åŠ  Provider
\`\`\`typescript
import { LlmConnectorProvider } from 'llm-connector';

<LlmConnectorProvider name="my-chat" storageKey="chat-config">
  <App />
</LlmConnectorProvider>
\`\`\`

### 3. ä½¿ç”¨ Hook
\`\`\`typescript
import { useLlmConnector } from 'llm-connector';

const { llmClient, states, handlers } = useLlmConnector('my-chat');

// é…ç½®
handlers.setProviderId('openai');
handlers.setApiKey('sk-...');
await handlers.handleConnect();

// å¯¹è¯
const result = await llmClient.chat({
  messages: [{ role: 'user', content: 'Hello!' }],
});
\`\`\`

## å„ Provider é…ç½®ç¤ºä¾‹

è§å®Œæ•´æ–‡æ¡£...
```

---

## âœ… éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½å®Œæ•´æ€§
- [ ] 4ä¸ªæ–° Provider å…¨éƒ¨å®ç°
- [ ] å‰ç«¯/åç«¯æ¨¡å¼åˆ‡æ¢æ­£å¸¸
- [ ] é…ç½®æŒä¹…åŒ–æ­£å¸¸
- [ ] å¤šå®ä¾‹éš”ç¦»æ­£å¸¸

### ä»£ç è´¨é‡
- [ ] TypeScript æ— ç±»å‹é”™è¯¯
- [ ] æ‰€æœ‰ Provider æœ‰å•å…ƒæµ‹è¯•
- [ ] ä»£ç è¦†ç›–ç‡ > 80%
- [ ] ESLint æ— é”™è¯¯

### æ–‡æ¡£å®Œæ•´æ€§
- [ ] API æ–‡æ¡£å®Œæ•´
- [ ] ä½¿ç”¨ç¤ºä¾‹æ¸…æ™°
- [ ] æ•…éšœæ’æŸ¥æŒ‡å—

### æ€§èƒ½
- [ ] é¦–æ¬¡è¿æ¥ < 3ç§’
- [ ] å¯¹è¯å“åº”å»¶è¿Ÿ < 1ç§’
- [ ] æ— å†…å­˜æ³„æ¼

---

## ğŸ“š ä¸‹ä¸€æ­¥

å®Œæˆåç»§ç»­åˆ›å»º:
1. **AI-Adapter-Integration.md** - è¯¦ç»†é›†æˆæŒ‡å—
2. **AI-Adapter-Testing.md** - æµ‹è¯•ç”¨ä¾‹å’Œè‡ªåŠ¨åŒ–
3. **AI-Adapter-Deployment.md** - éƒ¨ç½²å’Œè¿ç»´

---

**æ–‡æ¡£å®Œæˆ** âœ…
