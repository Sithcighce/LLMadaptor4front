# LLM前端连接器 - 让AI接入变得简单

## 📋 项目核心目标

### 1. 提供开箱即用的UI组件系统
- ✅ **已完成**：三个基础UI组件（连接表单、模型选择、Token使用显示）
- ✅ **已完成**：预设中英文支持，支持样式和文字调整
- 🔄 **进行中**：对话UI组件和其他扩展组件的开发

### 2. 构建通用的前端大模型调用路由
支持多层次的调用模式：
- ✅ **逻辑触发的简单调用** - 无UI的纯API调用
- 🔄 **对话中的调用** - 带历史记录的聊天交互
- 📋 **复杂调用** - RAG、Agent等高级功能

**设计理念**：适配各种层次的开发者需求（从开箱即用到复杂自定义配置）

### 3. 最大化服务商支持
- ✅ **已支持**：OpenAI、Anthropic、Google Gemini
- ✅ **已支持**：WebLLM（浏览器本地运行）
- 📋 **计划支持**：Google Chrome AI、LM Studio、其他本地模型

### 4. 打造交互式展示平台
- 📋 **目标**：将 `App.tsx` 页面改造为：
  - 带交互功能的在线demo
  - 集成使用说明和文档
  - 展示各种调用方式的实例

### 5. 多实例支持能力
- 🤔 **待确认**：在同一项目中创建多个LLM实例
  - 场景：总结用廉价模型，对话用高质量模型
  - 技术考虑：状态隔离、配置管理

### 6. 无头化客户端设计
- 🤔 **待确认**：client支持无历史记录的纯功能调用
- **优势**：适用于工具类、批处理等场景
🤔 为什么要做这个项目？

商业模式：
AI应用，很多时候不值得让用户订阅。可是免费给用户又太贵。自然希望让用户自带api。
甚至兼容webllm

开发者的烦恼
想象一下，你想做个简单的AI小工具，但是：

不想搭后端 - 太麻烦了，就想纯前端搞定
不想自己充值 - 让用户用自己的API Key不香吗？
不想管理用户密钥 - 责任太大，数据安全问题一堆
只想专心写业务逻辑，别的都不想碰

现在的解决方案要么是重型框架（像LobeChat这种完整应用），要么是纯SDK（需要自己写UI），要么需要后端支持。就是没有一个"拿来就用"的纯前端组件。
用户的痛苦
用户手里往往有各种来路的API Key：

OpenAI官方的（贵）
Claude官方的（也贵）
各种白嫖来的中介API - 便宜甚至免费，但是：

API端点地址千奇百怪
兼容性参差不齐
有些需要特殊的请求头
模型名称可能不一样



每换一个应用都要重新研究怎么配置，而且不同应用的界面还不一样，太折腾了。
现有方案的问题
市面上解决API适配的往往是后端方案（比如LiteLLM、OpenRouter），但这其实是个伪需求：

后端能选的模型都是开发者预先配置好的，相对固定
真正千奇百怪、需要适配的，是用户手里的各种密钥
这些密钥只有在前端才能真正发挥"用户自选"的价值

所以真正的适配挑战在前端，而不是后端。
💡 我们要做什么？
一个即插即用的React组件，专门解决"用户自带API Key + 纯前端调用"的场景。
核心要素：

🎛️ 下拉选择UI组件 - 支持各种服务商和自定义配置
🌐 纯前端调用 - 直接从浏览器调用LLM API，无需后端
💾 浏览器缓存 - 配置信息安全存储在用户本地

想象这样的使用体验：
jsx// 开发者：只需要这几行代码
<LlmConnector onReady={(client) => {
  // 用户配置好后，你就得到一个标准的client对象
  // 不管用户选的是GPT-4还是Claude，调用方式都一样
  setClient(client);
}} />

// 然后就可以愉快地聊天了
await client.chat({
  messages: [{ role: "user", content: "Hello!" }]
});
用户看到的是一个简洁的配置界面：

下拉选择服务商（OpenAI、Anthropic、自定义等）
输入API Key（只存在自己浏览器里）
如果是自定义的，还能填写API地址
自动获取可用模型列表
一键连接，搞定

🎯 核心价值
对开发者：

🚀 一行代码集成 - 不用再重复造轮子
🔧 统一接口 - 不管用户选什么模型，你的代码都不用改
💰 零成本 - 纯前端，不需要服务器

对用户：

🔐 绝对安全 - API Key只存在你自己的浏览器里
🎛️ 完全控制 - 想用哪个服务就用哪个，随时切换
💾 记住配置 - 配置一次，以后自动记住