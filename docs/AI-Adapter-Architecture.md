# AI API é€‚é…å™¨ - æ¶æ„è®¾è®¡ä¸æ”¹é€ æ–¹æ¡ˆ

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv1.0  
**åˆ›å»ºæ—¥æœŸ**ï¼š2025-10-20  
**åŸºäº**ï¼šllm-connector v0.3.2

---

## âš ï¸ æ ¸å¿ƒæ¶æ„ - ä¸‰ç§æ¨ç†æ¨¡å¼ç»Ÿä¸€

**æœ¬é¡¹ç›®æ˜¯ç»Ÿä¸€çš„ LLM è¿æ¥å™¨åº“,æ”¯æŒä¸‰ç§åŒç­‰é‡è¦çš„æ¨ç†æ¨¡å¼:**

### 1ï¸âƒ£ å‰ç«¯ BYOK (Bring Your Own Key)

```
ç”¨æˆ· â†’ å‰ç«¯ UI â†’ ç²˜è´´è‡ªå·±çš„ API Key â†’ localStorage â†’ å‰ç«¯ç›´æ¥è°ƒç”¨ AI Provider
```

- âœ… ç”¨æˆ·ç®¡ç†è‡ªå·±çš„å¯†é’¥
- âœ… å®Œå…¨åœ¨æµè§ˆå™¨è¿è¡Œ
- âœ… é›¶åç«¯ä¾èµ–

### 2ï¸âƒ£ ç«¯ä¾§æ¨ç† (On-Device Inference)

```
ç”¨æˆ· â†’ å‰ç«¯ UI â†’ Chrome AI / WebLLM â†’ æµè§ˆå™¨ç«¯æœ¬åœ°æ¨ç†
```

- âœ… æ— éœ€ API Key
- âœ… å®Œå…¨æœ¬åœ°è¿è¡Œ
- âœ… éšç§ä¿æŠ¤

### 3ï¸âƒ£ åç«¯ä»£ç† (Backend Proxy / Local Server)

```
ç”¨æˆ· â†’ å‰ç«¯ UI â†’ åç«¯æœåŠ¡å™¨ â†’ ç»Ÿä¸€æ·»åŠ  Key â†’ AI Provider / æœ¬åœ°æ¨¡å‹
```

- âœ… ä¼ä¸šç»Ÿä¸€ç®¡ç†å¯†é’¥
- âœ… æ”¯æŒæœ¬åœ°æ¨¡å‹æœåŠ¡å™¨
- âœ… é›†ä¸­æ§åˆ¶å’Œå®¡è®¡

**ä¸‰ç§æ¨¡å¼åŒç­‰é‡è¦,ç”¨æˆ·æ ¹æ®åœºæ™¯é€‰æ‹©ã€‚**

---

## ğŸ“‹ ç›®å½•

1. [ç°æœ‰æ¶æ„åˆ†æ](#ç°æœ‰æ¶æ„åˆ†æ)
2. [æ”¹é€ éœ€æ±‚](#æ”¹é€ éœ€æ±‚)
3. [æ–°æ¶æ„è®¾è®¡](#æ–°æ¶æ„è®¾è®¡)
4. [æ ¸å¿ƒæ”¹é€ ç‚¹](#æ ¸å¿ƒæ”¹é€ ç‚¹)
5. [æ•°æ®æµè®¾è®¡](#æ•°æ®æµè®¾è®¡)
6. [ç±»å‹ç³»ç»Ÿæ‰©å±•](#ç±»å‹ç³»ç»Ÿæ‰©å±•)

---

## ğŸ” ç°æœ‰æ¶æ„åˆ†æ

### llm-connector æ ¸å¿ƒæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LlmConnectorProvider (Context Provider)                â”‚
â”‚  â”œâ”€ ç®¡ç†å…¨å±€çŠ¶æ€ï¼ˆproviderId, apiKey, modelç­‰ï¼‰        â”‚
â”‚  â”œâ”€ æŒä¹…åŒ–åˆ° localStorage                               â”‚
â”‚  â””â”€ æä¾› Context ç»™å­ç»„ä»¶                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  useLlmConnectorLogic (æ ¸å¿ƒé€»è¾‘ Hook)                   â”‚
â”‚  â”œâ”€ çŠ¶æ€ç®¡ç†ï¼ˆstatus, error, llmClientç­‰ï¼‰             â”‚
â”‚  â”œâ”€ è¿æ¥å¤„ç†ï¼ˆhandleConnect, handleDisconnectï¼‰        â”‚
â”‚  â”œâ”€ æ¨¡å‹ç®¡ç†ï¼ˆfetchModels, setModelï¼‰                   â”‚
â”‚  â””â”€ Token ç»Ÿè®¡ï¼ˆupdateTokenUsageï¼‰                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Provider å±‚ (ç‰¹å®šå¹³å°å®ç°)                              â”‚
â”‚  â”œâ”€ OpenaiProvider.ts  â†’ åˆ›å»º TokenJS å®ä¾‹              â”‚
â”‚  â”œâ”€ AnthropicProvider.ts                                â”‚
â”‚  â”œâ”€ GeminiProvider.ts                                   â”‚
â”‚  â””â”€ WebLlmProvider.ts  â†’ ä½¿ç”¨ @mlc-ai/web-llm          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LlmClient (ç»Ÿä¸€å®¢æˆ·ç«¯æ¥å£)                              â”‚
â”‚  â””â”€ chat(request) â†’ è°ƒç”¨ TokenJS æˆ– WebLLM             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åº•å±‚å¼•æ“                                                â”‚
â”‚  â”œâ”€ Token.js â†’ æ”¯æŒ 200+ LLM API                       â”‚
â”‚  â””â”€ @mlc-ai/web-llm â†’ æµè§ˆå™¨å†… WASM æ¨ç†               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å…³é”®è®¾è®¡æ¨¡å¼

#### 1. Provider æ¨¡å¼
æ¯ä¸ª AI å¹³å°å®ç°ç‹¬ç«‹çš„ Providerï¼š

```typescript
// providers/OpenaiProvider.ts
export const createOpenaiProvider = (apiKey: string, baseUrl?: string) => {
  return new TokenJS({
    provider: 'openai',
    apiKey: apiKey,
    baseURL: baseUrl || 'https://api.openai.com/v1',
  });
};
```

#### 2. ç»Ÿä¸€å®¢æˆ·ç«¯æ¥å£
æ‰€æœ‰ Provider è¿”å›çš„å¯¹è±¡è¢«åŒ…è£…æˆ `LlmClient`ï¼š

```typescript
export class LlmClient {
  private tokenJs: TokenJSInterface;
  
  public async chat(request: ChatRequest): Promise<ChatResult> {
    const response = await this.tokenJs.chat.completions.create({
      provider: this.provider,
      model: this.model,
      messages: request.messages,
      stream: request.stream ?? false,
    });
    
    return {
      text: response.choices[0].message.content,
      usage: { input: ..., output: ... },
      stop_reason: response.choices[0].finish_reason,
    };
  }
}
```

#### 3. Context + Hook æ¨¡å¼
é€šè¿‡ React Context å…±äº«çŠ¶æ€ï¼š

```typescript
// åœ¨ Provider ä¸­æ³¨å†Œ
<LlmConnectorProvider name="chat" storageKey="chat-config">
  <App />
</LlmConnectorProvider>

// åœ¨ç»„ä»¶ä¸­ä½¿ç”¨
const { llmClient, states, handlers } = useLlmConnector('chat');
```

---

## ğŸ¯ æ”¹é€ éœ€æ±‚

### éœ€è¦æ–°å¢çš„åŠŸèƒ½

#### 1. æ–°å¢ Provider æ”¯æŒ
- **Chrome AI API**: æµè§ˆå™¨å†…ç½® AI
- **LM Studio**: æœ¬åœ° OpenAI å…¼å®¹æœåŠ¡å™¨
- **ç¡…åŸºæµåŠ¨**: å›½å†… API æä¾›å•†
- **åç«¯ä»£ç†**: é€šè¿‡åç«¯è½¬å‘è¯·æ±‚

#### 2. è°ƒç”¨æ¨¡å¼æ‰©å±•
**å½“å‰**: ä»…å‰ç«¯ç›´è°ƒï¼ˆAPI Key åœ¨æµè§ˆå™¨ï¼‰
**æ–°å¢**: æ”¯æŒåç«¯ä»£ç†ï¼ˆAPI Key åœ¨æœåŠ¡å™¨ï¼‰

```typescript
// å‰ç«¯ç›´è°ƒ
setCallMode('frontend');
setApiKey('user-provided-key');

// åç«¯ä»£ç†
setCallMode('backend');
setBackendUrl('/api/ai/proxy');
```

#### 3. é…ç½®çµæ´»æ€§
**å½“å‰**: å¿…é¡»é€šè¿‡ UI ç»„ä»¶é…ç½®
**æ–°å¢**: æ”¯æŒçº¯ä»£ç é…ç½®

```typescript
// ç¨‹åºåŒ–é…ç½®
const connector = useLlmConnectorLogic('my-config');
connector.setProviderId('chrome-ai');
connector.handleConnect();
```

---

## ğŸ—ï¸ æ–°æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åº”ç”¨å±‚                                                  â”‚
â”‚  â”œâ”€ Google Audio (STT + AI Chat)                       â”‚
â”‚  â””â”€ PDF Reader (é€æ®µè®²è§£ + æ–‡æœ¬é€‰æ‹©é—®ç­”)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ useLlmConnector('instance-name')
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LlmConnectorProvider (æ‰©å±•)                            â”‚
â”‚  â”œâ”€ æ–°å¢ï¼šcallMode ('frontend' | 'backend')            â”‚
â”‚  â”œâ”€ æ–°å¢ï¼šbackendUrl (åç«¯ä»£ç†åœ°å€)                     â”‚
â”‚  â””â”€ ä¿ç•™ï¼šåŸæœ‰æ‰€æœ‰åŠŸèƒ½                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  useLlmConnectorLogic (æ‰©å±•æ ¸å¿ƒé€»è¾‘)                     â”‚
â”‚  â”œâ”€ åŸæœ‰ï¼šproviderId, apiKey, model, status            â”‚
â”‚  â”œâ”€ æ–°å¢ï¼šcallMode, backendUrl                         â”‚
â”‚  â”œâ”€ æ–°å¢ï¼šProvider é€‰æ‹©é€»è¾‘ï¼ˆæ ¹æ® callModeï¼‰            â”‚
â”‚  â””â”€ æ–°å¢ï¼šåç«¯å¯ç”¨æ€§æ£€æµ‹                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Provider å±‚ (æ‰©å±•)                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  å‰ç«¯ç›´è°ƒ Providers  â”‚  åç«¯ä»£ç† Provider          â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ âœ… OpenAI           â”‚ ğŸ†• BackendProxyProvider    â”‚ â”‚
â”‚  â”‚ âœ… Anthropic        â”‚   (ç»Ÿä¸€åç«¯è°ƒç”¨)           â”‚ â”‚
â”‚  â”‚ âœ… Gemini           â”‚                             â”‚ â”‚
â”‚  â”‚ âœ… WebLLM           â”‚                             â”‚ â”‚
â”‚  â”‚ ğŸ†• ChromeAI         â”‚                             â”‚ â”‚
â”‚  â”‚ ğŸ†• LMStudio         â”‚                             â”‚ â”‚
â”‚  â”‚ ğŸ†• SiliconFlow      â”‚                             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LlmClient (ä¿æŒä¸å˜)                                    â”‚
â”‚  â””â”€ ç»Ÿä¸€çš„ chat(request) æ¥å£                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å®é™… AI åç«¯                                            â”‚
â”‚  â”œâ”€ æµè§ˆå™¨ï¼šChrome AI, WebLLM                          â”‚
â”‚  â”œâ”€ æœ¬åœ°ï¼šLM Studio (localhost:1234)                   â”‚
â”‚  â”œâ”€ äº‘ç«¯ï¼šOpenAI, Anthropic, ç¡…åŸºæµåŠ¨                   â”‚
â”‚  â””â”€ ä»£ç†ï¼š/api/ai/proxy (Next.js API Route)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ æ ¸å¿ƒæ”¹é€ ç‚¹

### æ”¹é€ ç‚¹1ï¼šæ‰©å±•çŠ¶æ€ç®¡ç†

**æ–‡ä»¶**: `src/hooks/useLlmConnectorLogic.ts`

**æ–°å¢çŠ¶æ€**:
```typescript
export const useLlmConnectorLogic = (storageKey: string = 'llm-connector-config') => {
  // === åŸæœ‰çŠ¶æ€ï¼ˆä¿ç•™ï¼‰===
  const [providerId, setProviderId] = useState<ProviderId>('openai');
  const [apiKey, setApiKey] = useState('');
  const [baseUrl, setBaseUrl] = useState('');
  const [model, setModel] = useState('gpt-4o');
  const [status, setStatus] = useState<ConnectorStatus>('disconnected');
  
  // === æ–°å¢çŠ¶æ€ ===
  const [callMode, setCallMode] = useState<CallMode>('frontend'); // ğŸ†•
  const [backendUrl, setBackendUrl] = useState('/api/ai/proxy');  // ğŸ†•
  const [backendAvailable, setBackendAvailable] = useState(false); // ğŸ†•
  
  // ... å…¶ä»–é€»è¾‘
};
```

**æ–°å¢æŒä¹…åŒ–**:
```typescript
// Load config from localStorage
useEffect(() => {
  try {
    const savedConfig = localStorage.getItem(storageKey);
    if (savedConfig) {
      const { 
        providerId, 
        baseUrl, 
        model,
        callMode,    // ğŸ†•
        backendUrl   // ğŸ†•
      } = JSON.parse(savedConfig);
      
      if (providerId) setProviderId(providerId);
      if (baseUrl) setBaseUrl(baseUrl);
      if (model) setModel(model);
      if (callMode) setCallMode(callMode);         // ğŸ†•
      if (backendUrl) setBackendUrl(backendUrl);   // ğŸ†•
    }
  } catch (e) {
    console.error('Failed to load config', e);
  }
}, [storageKey]);

// Save config to localStorage
useEffect(() => {
  try {
    const configToSave = { 
      providerId, 
      baseUrl, 
      model,
      callMode,    // ğŸ†•
      backendUrl   // ğŸ†•
    };
    localStorage.setItem(storageKey, JSON.stringify(configToSave));
  } catch (e) {
    console.error('Failed to save config', e);
  }
}, [providerId, baseUrl, model, callMode, backendUrl, storageKey]);
```

---

### æ”¹é€ ç‚¹2ï¼šæ–°å¢ Provider å®ç°

#### 2.1 ChromeAIProvider

**æ–‡ä»¶**: `src/providers/ChromeAIProvider.ts`

```typescript
import { LlmClient } from '../client/LlmClient';

/**
 * Chrome AI Provider
 * ä½¿ç”¨æµè§ˆå™¨å†…ç½®çš„ window.ai API
 */
export const createChromeAIProvider = async (): Promise<LlmClient> => {
  // æ£€æŸ¥ Chrome AI å¯ç”¨æ€§
  if (!('ai' in window) || !window.ai || !window.ai.assistant) {
    throw new Error('Chrome AI is not available in this browser');
  }
  
  // æ£€æŸ¥èƒ½åŠ›
  const capabilities = await window.ai.assistant.capabilities();
  if (capabilities.available !== 'readily') {
    throw new Error(`Chrome AI is not ready: ${capabilities.available}`);
  }
  
  // åˆ›å»ºä¼šè¯
  const session = await window.ai.assistant.create({
    temperature: 0.7,
    topK: 3,
  });
  
  // åˆ›å»ºå…¼å®¹ TokenJS æ¥å£çš„åŒ…è£…å™¨
  const chromeAIWrapper = {
    chat: {
      completions: {
        create: async (params: any) => {
          const messages = params.messages;
          const lastMessage = messages[messages.length - 1];
          
          // Chrome AI å½“å‰åªæ”¯æŒå•è½®å¯¹è¯
          const response = await session.prompt(lastMessage.content);
          
          return {
            choices: [{
              message: {
                content: response,
              },
              finish_reason: 'stop',
            }],
            usage: {
              prompt_tokens: 0,  // Chrome AI ä¸æä¾› token ç»Ÿè®¡
              completion_tokens: 0,
            },
          };
        },
      },
    },
  };
  
  return new LlmClient(chromeAIWrapper, 'chrome-ai', 'chrome-ai-model');
};

// ç±»å‹æ‰©å±•
declare global {
  interface Window {
    ai?: {
      assistant: {
        capabilities: () => Promise<{
          available: 'readily' | 'after-download' | 'no';
        }>;
        create: (options?: {
          temperature?: number;
          topK?: number;
        }) => Promise<{
          prompt: (text: string) => Promise<string>;
          promptStreaming: (text: string) => AsyncIterable<string>;
          destroy: () => void;
        }>;
      };
    };
  }
}
```

#### 2.2 LMStudioProvider

**æ–‡ä»¶**: `src/providers/LMStudioProvider.ts`

```typescript
import { TokenJS } from 'token.js';
import { LlmClient } from '../client/LlmClient';

/**
 * LM Studio Provider
 * è¿æ¥æœ¬åœ°è¿è¡Œçš„ LM Studio æœåŠ¡å™¨
 * é»˜è®¤ç«¯ç‚¹: http://localhost:1234/v1
 */
export const createLMStudioProvider = async (
  baseUrl: string = 'http://localhost:1234/v1',
  model?: string
): Promise<LlmClient> => {
  // æµ‹è¯•è¿æ¥
  try {
    const response = await fetch(`${baseUrl}/models`, {
      headers: { 'Content-Type': 'application/json' },
    });
    
    if (!response.ok) {
      throw new Error(`LM Studio not responding at ${baseUrl}`);
    }
    
    const data = await response.json();
    const models = data.data || [];
    
    if (models.length === 0) {
      throw new Error('No models loaded in LM Studio');
    }
    
    // ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹æˆ–ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹
    const selectedModel = model || models[0].id;
    
    // LM Studio å…¼å®¹ OpenAI APIï¼Œä½¿ç”¨ TokenJS
    const tokenJs = new TokenJS({
      provider: 'openai',
      apiKey: 'lm-studio',  // LM Studio ä¸éœ€è¦çœŸå® API Key
      baseURL: baseUrl,
    });
    
    return new LlmClient(tokenJs, 'lmstudio', selectedModel);
    
  } catch (error) {
    throw new Error(`Failed to connect to LM Studio: ${error.message}`);
  }
};
```

#### 2.3 SiliconFlowProvider

**æ–‡ä»¶**: `src/providers/SiliconFlowProvider.ts`

```typescript
import { TokenJS } from 'token.js';
import { LlmClient } from '../client/LlmClient';

/**
 * ç¡…åŸºæµåŠ¨ Provider
 * åŸºäº OpenAI å…¼å®¹æ¥å£
 */
export const createSiliconFlowProvider = (
  apiKey: string,
  model: string = 'Qwen/Qwen2.5-7B-Instruct'
): LlmClient => {
  const tokenJs = new TokenJS({
    provider: 'openai',
    apiKey: apiKey,
    baseURL: 'https://api.siliconflow.cn/v1',
  });
  
  return new LlmClient(tokenJs, 'siliconflow', model);
};
```

#### 2.4 BackendProxyProvider

**æ–‡ä»¶**: `src/providers/BackendProxyProvider.ts`

```typescript
import { LlmClient } from '../client/LlmClient';

/**
 * åç«¯ä»£ç† Provider
 * é€šè¿‡åç«¯ API è½¬å‘è¯·æ±‚ï¼ŒAPI Key å­˜å‚¨åœ¨æœåŠ¡å™¨
 */
export const createBackendProxyProvider = async (
  backendUrl: string = '/api/ai/proxy'
): Promise<LlmClient> => {
  // æµ‹è¯•åç«¯å¯ç”¨æ€§
  try {
    const response = await fetch(`${backendUrl}/health`, {
      method: 'GET',
    });
    
    if (!response.ok) {
      throw new Error(`Backend proxy not available at ${backendUrl}`);
    }
  } catch (error) {
    throw new Error(`Failed to connect to backend proxy: ${error.message}`);
  }
  
  // åˆ›å»ºåç«¯è°ƒç”¨åŒ…è£…å™¨
  const backendWrapper = {
    chat: {
      completions: {
        create: async (params: any) => {
          const response = await fetch(`${backendUrl}/chat`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              messages: params.messages,
              model: params.model,
              stream: params.stream || false,
              temperature: params.temperature,
              max_tokens: params.max_tokens,
            }),
          });
          
          if (!response.ok) {
            const error = await response.json();
            throw new Error(error.message || 'Backend proxy error');
          }
          
          return response.json();
        },
      },
    },
  };
  
  return new LlmClient(backendWrapper, 'backend-proxy', 'backend-model');
};
```

---

### æ”¹é€ ç‚¹3ï¼šæ‰©å±•è¿æ¥é€»è¾‘

**æ–‡ä»¶**: `src/hooks/useLlmConnectorLogic.ts`

**ä¿®æ”¹ `handleConnect` å‡½æ•°**:

```typescript
const handleConnect = useCallback(async () => {
  setStatus('connecting');
  setError(null);
  
  try {
    let client: LlmClient;
    
    // æ ¹æ® callMode å’Œ providerId é€‰æ‹© Provider
    if (callMode === 'backend') {
      // åç«¯ä»£ç†æ¨¡å¼
      client = await createBackendProxyProvider(backendUrl);
      
    } else {
      // å‰ç«¯ç›´è°ƒæ¨¡å¼
      switch (providerId) {
        case 'openai':
          client = createOpenaiProvider(apiKey, baseUrl);
          break;
          
        case 'anthropic':
          client = createAnthropicProvider(apiKey);
          break;
          
        case 'gemini':
          client = createGeminiProvider(apiKey);
          break;
          
        case 'webllm':
          client = await createWebLlmProvider(model);
          break;
          
        case 'chrome-ai':  // ğŸ†•
          client = await createChromeAIProvider();
          break;
          
        case 'lmstudio':   // ğŸ†•
          client = await createLMStudioProvider(baseUrl, model);
          break;
          
        case 'siliconflow': // ğŸ†•
          client = createSiliconFlowProvider(apiKey, model);
          break;
          
        default:
          throw new Error(`Unknown provider: ${providerId}`);
      }
    }
    
    setLlmClient(client);
    setStatus('connected');
    
  } catch (err) {
    setError(err as Error);
    setStatus('disconnected');
  }
}, [callMode, providerId, apiKey, baseUrl, model, backendUrl]);
```

---

## ğŸ“Š æ•°æ®æµè®¾è®¡

### å‰ç«¯ç›´è°ƒæµç¨‹

```
ç”¨æˆ·é…ç½®
  â”œâ”€ setProviderId('openai')
  â”œâ”€ setApiKey('sk-...')
  â”œâ”€ setCallMode('frontend')
  â””â”€ handleConnect()
      â†“
Provider é€‰æ‹©
  â”œâ”€ createOpenaiProvider(apiKey, baseUrl)
  â””â”€ è¿”å› TokenJS å®ä¾‹
      â†“
LlmClient åŒ…è£…
  â””â”€ new LlmClient(tokenJs, 'openai', 'gpt-4o')
      â†“
åº”ç”¨å±‚è°ƒç”¨
  â””â”€ llmClient.chat({ messages: [...] })
      â†“
TokenJS æ‰§è¡Œ
  â””â”€ ç›´æ¥è°ƒç”¨ OpenAI API
      â†“
è¿”å›ç»“æœ
  â””â”€ { text, usage, stop_reason }
```

### åç«¯ä»£ç†æµç¨‹

```
ç”¨æˆ·é…ç½®
  â”œâ”€ setCallMode('backend')
  â”œâ”€ setBackendUrl('/api/ai/proxy')
  â””â”€ handleConnect()
      â†“
åç«¯å¯ç”¨æ€§æ£€æµ‹
  â””â”€ fetch('/api/ai/proxy/health')
      â†“
BackendProxyProvider
  â””â”€ åˆ›å»ºåç«¯è°ƒç”¨åŒ…è£…å™¨
      â†“
LlmClient åŒ…è£…
  â””â”€ new LlmClient(backendWrapper, 'backend-proxy', 'backend-model')
      â†“
åº”ç”¨å±‚è°ƒç”¨
  â””â”€ llmClient.chat({ messages: [...] })
      â†“
å‘é€åˆ°åç«¯
  â””â”€ POST /api/ai/proxy/chat
      â†“
åç«¯å¤„ç†
  â”œâ”€ è¯»å–ç¯å¢ƒå˜é‡ä¸­çš„ API Key
  â”œâ”€ è°ƒç”¨å®é™… LLM API
  â””â”€ è¿”å›æ ‡å‡†åŒ–å“åº”
      â†“
è¿”å›ç»“æœ
  â””â”€ { text, usage, stop_reason }
```

---

## ğŸ¨ ç±»å‹ç³»ç»Ÿæ‰©å±•

### æ–°å¢ç±»å‹å®šä¹‰

**æ–‡ä»¶**: `src/types/index.ts`

```typescript
// === åŸæœ‰ç±»å‹ï¼ˆä¿ç•™ï¼‰===
export type ProviderId = 
  | 'openai' 
  | 'anthropic' 
  | 'gemini' 
  | 'webllm';

export type ConnectorStatus = 
  | 'disconnected' 
  | 'connecting' 
  | 'connected' 
  | 'error';

// === æ–°å¢ç±»å‹ ===

/**
 * è°ƒç”¨æ¨¡å¼
 * - frontend: å‰ç«¯ç›´è°ƒï¼ˆAPI Key åœ¨æµè§ˆå™¨ï¼‰
 * - backend: åç«¯ä»£ç†ï¼ˆAPI Key åœ¨æœåŠ¡å™¨ï¼‰
 */
export type CallMode = 'frontend' | 'backend';

/**
 * æ‰©å±• Provider ID
 */
export type ExtendedProviderId = ProviderId
  | 'chrome-ai'      // æµè§ˆå™¨å†…ç½® AI
  | 'lmstudio'       // æœ¬åœ° LM Studio
  | 'siliconflow'    // ç¡…åŸºæµåŠ¨
  | 'backend-proxy'; // åç«¯ä»£ç†

/**
 * Provider é…ç½®
 */
export interface ProviderConfig {
  id: ExtendedProviderId;
  name: string;
  requiresApiKey: boolean;
  requiresBaseUrl: boolean;
  supportsFrontend: boolean;
  supportsBackend: boolean;
  defaultModel?: string;
}

/**
 * è¿æ¥å™¨çŠ¶æ€ï¼ˆæ‰©å±•ï¼‰
 */
export interface ConnectorState {
  // åŸæœ‰çŠ¶æ€
  providerId: ExtendedProviderId;
  apiKey: string;
  baseUrl: string;
  model: string;
  status: ConnectorStatus;
  error: Error | null;
  modelOptions: string[];
  tokenUsage: TokenUsage | null;
  
  // æ–°å¢çŠ¶æ€
  callMode: CallMode;
  backendUrl: string;
  backendAvailable: boolean;
}

/**
 * è¿æ¥å™¨å¤„ç†å™¨ï¼ˆæ‰©å±•ï¼‰
 */
export interface ConnectorHandlers {
  // åŸæœ‰å¤„ç†å™¨
  setProviderId: (id: ExtendedProviderId) => void;
  setApiKey: (key: string) => void;
  setBaseUrl: (url: string) => void;
  setModel: (model: string) => void;
  handleConnect: () => Promise<void>;
  handleDisconnect: () => void;
  fetchModels: () => Promise<void>;
  
  // æ–°å¢å¤„ç†å™¨
  setCallMode: (mode: CallMode) => void;
  setBackendUrl: (url: string) => void;
  checkBackendAvailability: () => Promise<boolean>;
}
```

### Provider æ³¨å†Œè¡¨

**æ–‡ä»¶**: `src/registry/providers.ts`

```typescript
import type { ProviderConfig } from '../types';

export const PROVIDER_REGISTRY: Record<ExtendedProviderId, ProviderConfig> = {
  openai: {
    id: 'openai',
    name: 'OpenAI',
    requiresApiKey: true,
    requiresBaseUrl: false,
    supportsFrontend: true,
    supportsBackend: true,
    defaultModel: 'gpt-4o',
  },
  anthropic: {
    id: 'anthropic',
    name: 'Anthropic',
    requiresApiKey: true,
    requiresBaseUrl: false,
    supportsFrontend: true,
    supportsBackend: true,
    defaultModel: 'claude-3-5-sonnet-20241022',
  },
  gemini: {
    id: 'gemini',
    name: 'Google Gemini',
    requiresApiKey: true,
    requiresBaseUrl: false,
    supportsFrontend: true,
    supportsBackend: true,
    defaultModel: 'gemini-pro',
  },
  webllm: {
    id: 'webllm',
    name: 'WebLLM',
    requiresApiKey: false,
    requiresBaseUrl: false,
    supportsFrontend: true,
    supportsBackend: false,
    defaultModel: 'Llama-3.1-8B-Instruct-q4f16_1-MLC',
  },
  'chrome-ai': {
    id: 'chrome-ai',
    name: 'Chrome AI',
    requiresApiKey: false,
    requiresBaseUrl: false,
    supportsFrontend: true,
    supportsBackend: false,
  },
  lmstudio: {
    id: 'lmstudio',
    name: 'LM Studio',
    requiresApiKey: false,
    requiresBaseUrl: true,
    supportsFrontend: true,
    supportsBackend: false,
    defaultModel: 'auto',
  },
  siliconflow: {
    id: 'siliconflow',
    name: 'ç¡…åŸºæµåŠ¨',
    requiresApiKey: true,
    requiresBaseUrl: false,
    supportsFrontend: true,
    supportsBackend: true,
    defaultModel: 'Qwen/Qwen2.5-7B-Instruct',
  },
  'backend-proxy': {
    id: 'backend-proxy',
    name: 'Backend Proxy',
    requiresApiKey: false,
    requiresBaseUrl: true,
    supportsFrontend: false,
    supportsBackend: true,
  },
};
```

---

## ğŸ” å®‰å…¨æ€§è€ƒè™‘

### å‰ç«¯ç›´è°ƒæ¨¡å¼
- âš ï¸ API Key å­˜å‚¨åœ¨ localStorageï¼ˆæ˜æ–‡ï¼‰
- âš ï¸ ç½‘ç»œè¯·æ±‚å¯è¢«æ‹¦æˆª
- âœ… é€‚ç”¨äºå¼€å‘/æµ‹è¯•ç¯å¢ƒ
- âœ… ç”¨æˆ·è‡ªå·±çš„ API Keyï¼Œè´£ä»»è‡ªè´Ÿ

### åç«¯ä»£ç†æ¨¡å¼
- âœ… API Key å­˜å‚¨åœ¨æœåŠ¡å™¨ç¯å¢ƒå˜é‡
- âœ… å‰ç«¯æ— æ³•è®¿é—®çœŸå® API Key
- âœ… å¯æ·»åŠ é€Ÿç‡é™åˆ¶å’Œé‰´æƒ
- âœ… é€‚ç”¨äºç”Ÿäº§ç¯å¢ƒ

### æœ€ä½³å®è·µ
```typescript
// ç”Ÿäº§ç¯å¢ƒæ¨èé…ç½®
<LlmConnectorProvider
  name="production-chat"
  storageKey="chat-config"
  defaultCallMode="backend"  // é»˜è®¤ä½¿ç”¨åç«¯ä»£ç†
  defaultBackendUrl="/api/ai/proxy"
>
  <App />
</LlmConnectorProvider>
```

---

## âœ… æ”¹é€ éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½å®Œæ•´æ€§
- [ ] æ‰€æœ‰æ–° Provider æ­£å¸¸å·¥ä½œ
- [ ] å‰ç«¯ç›´è°ƒæ¨¡å¼æ­£å¸¸
- [ ] åç«¯ä»£ç†æ¨¡å¼æ­£å¸¸
- [ ] é…ç½®æŒä¹…åŒ–æ­£å¸¸
- [ ] å¤šå®ä¾‹éš”ç¦»æ­£å¸¸

### å…¼å®¹æ€§
- [ ] ä¸ç ´ååŸæœ‰ llm-connector åŠŸèƒ½
- [ ] æ‰€æœ‰åŸæœ‰ Provider ä»ç„¶å·¥ä½œ
- [ ] API å‘åå…¼å®¹

### ä»£ç è´¨é‡
- [ ] TypeScript ç±»å‹æ£€æŸ¥é€šè¿‡
- [ ] ä»£ç é£æ ¼ä¸€è‡´
- [ ] æœ‰å……åˆ†çš„é”™è¯¯å¤„ç†
- [ ] æœ‰å¿…è¦çš„æ³¨é‡Š

---

**æ–‡æ¡£å®Œæˆ** âœ…  
**ä¸‹ä¸€æ­¥**ï¼šè¯¦ç»†å®æ–½æŒ‡å—ï¼ˆAI-Adapter-Implementation.mdï¼‰
