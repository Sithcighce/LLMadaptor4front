# Vercel AI SDK é›†æˆé¢„ç ”æŠ¥å‘Š

**ç ”ç©¶æ—¥æœŸ**ï¼š2025-10-20  
**é¡¹ç›®**ï¼šLLM Connector  
**ç›®çš„**ï¼šè¯„ä¼°é›†æˆæˆ–å€Ÿé‰´ Vercel AI SDK çš„å¯è¡Œæ€§

---

## 1. Vercel AI SDK æ¦‚è¿°

### æ ¸å¿ƒç‰¹æ€§
- **ç»Ÿä¸€æ¥å£** - æ”¯æŒ OpenAI, Anthropic, Google, Mistral ç­‰
- **æµå¼å“åº”** - åŸç”Ÿæ”¯æŒ SSE å’Œ React Stream
- **React Hooks** - useChat, useCompletion, useAssistant
- **è¾¹ç¼˜è¿è¡Œ** - Vercel Edge Runtime ä¼˜åŒ–
- **ç±»å‹å®‰å…¨** - å®Œæ•´ TypeScript æ”¯æŒ

### æ¶æ„æ¨¡å¼
```
React App
  â†“ useChat()
AI SDK UI (å‰ç«¯)
  â†“ fetch('/api/chat')
AI SDK Core (åç«¯)
  â†“ createOpenAI()
Provider API
```

---

## 2. ä¸ LLM Connector å¯¹æ¯”

### ç›¸ä¼¼ä¹‹å¤„

| ç‰¹æ€§ | LLM Connector | Vercel AI SDK | å¤‡æ³¨ |
|------|--------------|---------------|------|
| ç»Ÿä¸€æ¥å£ | âœ… LlmClient | âœ… LanguageModel | éƒ½æä¾›ç»Ÿä¸€æŠ½è±¡ |
| å¤š Provider | âœ… 8 ä¸ª | âœ… 10+ | éƒ½æ”¯æŒä¸»æµæœåŠ¡ |
| æµå¼å“åº” | âœ… SSE | âœ… Stream | å®ç°æ–¹å¼ç±»ä¼¼ |
| React Hooks | âœ… useLlmConnector | âœ… useChat | éƒ½æä¾› Hook |
| TypeScript | âœ… å®Œæ•´ | âœ… å®Œæ•´ | ç±»å‹éƒ½å¾ˆå¥½ |

### æ ¸å¿ƒå·®å¼‚

| ç»´åº¦ | LLM Connector | Vercel AI SDK | å½±å“ |
|------|--------------|---------------|------|
| **éƒ¨ç½²æ¨¡å‹** | å‰ç«¯ç›´è¿ | åç«¯ä»£ç† | ğŸ”´ æ¶æ„å·®å¼‚å¤§ |
| **API Key** | å‰ç«¯ç®¡ç† | åç«¯ç®¡ç† | ğŸ”´ å®‰å…¨æ¨¡å‹ä¸åŒ |
| **ç›®æ ‡åœºæ™¯** | BYOK åº”ç”¨ | SaaS åº”ç”¨ | ğŸ”´ ç”¨æˆ·ç¾¤ä¸åŒ |
| **æµè§ˆå™¨ AI** | âœ… æ”¯æŒ | âŒ ä¸æ”¯æŒ | ğŸŸ¢ æˆ‘ä»¬æœ‰ä¼˜åŠ¿ |
| **å¤šå®ä¾‹** | âœ… ClientRegistry | âŒ å•ä¾‹ | ğŸŸ¢ æˆ‘ä»¬æ›´çµæ´» |
| **UI ç»„ä»¶** | âœ… ç‹¬ç«‹ç»„ä»¶ | âš ï¸ Hook ç»‘å®š | ğŸŸ¡ å„æœ‰ä¼˜åŠ£ |

---

## 3. é›†æˆå¯è¡Œæ€§åˆ†æ

### æ–¹æ¡ˆ Aï¼šå®Œå…¨æ›¿æ¢ï¼ˆä¸æ¨è âŒï¼‰

**åšæ³•**ï¼šæ”¾å¼ƒ LLM Connectorï¼Œç›´æ¥ç”¨ Vercel AI SDK

**ä¼˜ç‚¹**ï¼š
- ç¤¾åŒºæ”¯æŒå¼º
- æ–‡æ¡£å®Œå–„
- æŒç»­æ›´æ–°

**ç¼ºç‚¹**ï¼š
- âŒ **æ¶æ„ä¸å…¼å®¹** - å¿…é¡»æœ‰åç«¯
- âŒ **ä¸§å¤±æ ¸å¿ƒä¼˜åŠ¿** - BYOK æ¨¡å¼
- âŒ **ä¸æ”¯æŒæµè§ˆå™¨ AI** - Chrome AI, WebLLM
- âŒ **å¤šå®ä¾‹æ”¯æŒå·®**
- âŒ **å‰æœŸæŠ•å…¥ç™½è´¹**

**ç»“è®º**ï¼šâŒ **ä¸å¯è¡Œ**

---

### æ–¹æ¡ˆ Bï¼šåŒæ¨¡å¼é›†æˆï¼ˆå¤æ‚ âš ï¸ï¼‰

**åšæ³•**ï¼šåŒæ—¶æ”¯æŒä¸¤ç§æ¨¡å¼
- å‰ç«¯ç›´è¿æ¨¡å¼ - ä¿ç•™ç°æœ‰æ¶æ„
- åç«¯ä»£ç†æ¨¡å¼ - é›†æˆ Vercel AI SDK

**ä¼˜ç‚¹**ï¼š
- å…¼é¡¾ä¸¤ç§åœºæ™¯
- ç”¨æˆ·å¯é€‰æ‹©

**ç¼ºç‚¹**ï¼š
- âš ï¸ **ç»´æŠ¤æˆæœ¬ç¿»å€**
- âš ï¸ **API ä¸ç»Ÿä¸€**
- âš ï¸ **æµ‹è¯•å¤æ‚åº¦é«˜**
- âš ï¸ **æ–‡æ¡£å·¥ä½œé‡å¤§**

**ç»“è®º**ï¼šâš ï¸ **ä¸æ¨è** - é™¤éæœ‰æ˜ç¡®éœ€æ±‚

---

### æ–¹æ¡ˆ Cï¼šAPI è®¾è®¡å€Ÿé‰´ï¼ˆæ¨è âœ…ï¼‰

**åšæ³•**ï¼šå­¦ä¹  Vercel AI SDK çš„ä¼˜ç§€è®¾è®¡ï¼Œæ”¹è¿›æˆ‘ä»¬çš„ API

#### å¯å€Ÿé‰´çš„è®¾è®¡

##### 1. Hook API è®¾è®¡
```typescript
// Vercel AI SDK - ç®€æ´ä¼˜é›…
const { messages, input, handleSubmit, isLoading } = useChat();

// LLM Connector - å½“å‰
const { states, handlers, llmClient } = useLlmConnector();
const { messages, sendMessage, isStreaming } = useChatManager();

// å»ºè®®ï¼šåˆå¹¶ä¸ºå•ä¸€ Hook
const { 
  messages, 
  input, 
  setInput,
  submit,
  isLoading,
  error 
} = useLlmChat({ clientName: 'my-chat' });
```

##### 2. æµå¼å“åº” API
```typescript
// Vercel AI SDK - ç»Ÿä¸€æ ¼å¼
const stream = await streamText({
  model: openai('gpt-4'),
  messages,
});

for await (const chunk of stream.textStream) {
  process.stdout.write(chunk);
}

// LLM Connector - å¯æ”¹è¿›
// ç»Ÿä¸€æ‰€æœ‰ Provider çš„æµå¼å“åº”æ ¼å¼
```

##### 3. é”™è¯¯å¤„ç†
```typescript
// Vercel AI SDK - ç»“æ„åŒ–é”™è¯¯
try {
  await generateText();
} catch (error) {
  if (error instanceof APIError) {
    console.log(error.statusCode, error.message);
  }
}

// å»ºè®®ï¼šåˆ›å»ºç»Ÿä¸€çš„é”™è¯¯ç±»å‹
```

##### 4. å·¥å…·è°ƒç”¨ï¼ˆFunction Callingï¼‰
```typescript
// Vercel AI SDK - ä¼˜é›…çš„å·¥å…·å®šä¹‰
const result = await generateText({
  model: openai('gpt-4'),
  tools: {
    weather: {
      description: 'Get the weather',
      parameters: z.object({
        location: z.string(),
      }),
      execute: async ({ location }) => getWeather(location),
    },
  },
});

// å»ºè®®ï¼šæœªæ¥å¯æ·»åŠ æ­¤åŠŸèƒ½
```

#### ä¸é€‚åˆå€Ÿé‰´çš„è®¾è®¡

1. âŒ **åç«¯ä¾èµ–** - æˆ‘ä»¬æ˜¯å‰ç«¯ç›´è¿
2. âŒ **Route Handler** - æˆ‘ä»¬ä¸éœ€è¦ API è·¯ç”±
3. âŒ **Edge Runtime** - æˆ‘ä»¬åœ¨æµè§ˆå™¨è¿è¡Œ

---

### æ–¹æ¡ˆ Dï¼šæ··åˆæ¶æ„ï¼ˆæ¨è âœ…ï¼‰

**åšæ³•**ï¼šä¿æŒç°æœ‰æ¶æ„ï¼ŒæŒ‰éœ€æ·»åŠ  Vercel AI SDK å…¼å®¹å±‚

#### å®æ–½æ­¥éª¤

##### Phase 1ï¼šAPI ä¼˜åŒ–ï¼ˆç«‹å³ï¼‰
```typescript
// åˆ›å»ºç®€åŒ–ç‰ˆ Hook
export const useLlmChat = (config?: {
  clientName?: string;
  systemPrompt?: string;
}) => {
  const { llmClient } = useLlmConnector(config?.clientName);
  const { messages, sendMessage, isStreaming, error } = useChatManager(config);
  
  const [input, setInput] = useState('');
  
  const handleSubmit = useCallback((e: FormEvent) => {
    e.preventDefault();
    if (input.trim()) {
      sendMessage(input);
      setInput('');
    }
  }, [input, sendMessage]);
  
  return {
    messages,
    input,
    setInput,
    handleSubmit,
    isLoading: isStreaming,
    error,
  };
};
```

##### Phase 2ï¼šBackend Proxy å¢å¼ºï¼ˆä¸­æœŸï¼‰
```typescript
// åœ¨ backend-proxy ä¸­æ·»åŠ  Vercel AI SDK å…¼å®¹
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';

app.post('/api/chat', async (req, res) => {
  const { messages, provider } = req.body;
  
  // æ”¯æŒ Vercel AI SDK æ ¼å¼
  if (provider === 'vercel-compatible') {
    const result = await streamText({
      model: openai('gpt-4'),
      messages,
    });
    
    return result.toAIStreamResponse();
  }
  
  // ä¿ç•™ç°æœ‰é€»è¾‘...
});
```

##### Phase 3ï¼šæ’ä»¶ç³»ç»Ÿï¼ˆé•¿æœŸï¼‰
```typescript
// å…è®¸ç”¨æˆ·é€‰æ‹©åº•å±‚å¼•æ“
const connector = new LlmConnector({
  engine: 'tokenjs', // æˆ– 'vercel-ai-sdk'
  provider: 'openai',
});
```

---

## 4. å…·ä½“å»ºè®®

### ç«‹å³æ‰§è¡Œï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰

#### 4.1 ä¼˜åŒ– Hook API
- åˆ›å»º `useLlmChat` - ç®€åŒ–çš„èŠå¤© Hook
- åˆå¹¶ `useLlmConnector` å’Œ `useChatManager`
- æä¾›ç±»ä¼¼ Vercel AI SDK çš„ç®€æ´ API

```typescript
// æ–°å¢æ–‡ä»¶ï¼šsrc/hooks/useLlmChat.ts
// é‡æ–°å¯¼å‡ºï¼šsrc/index.tsx æ·»åŠ  useLlmChat
```

#### 4.2 ç»Ÿä¸€é”™è¯¯å¤„ç†
- åˆ›å»º `LlmError` åŸºç±»
- åŒºåˆ†ä¸åŒç±»å‹é”™è¯¯ï¼ˆç½‘ç»œã€APIã€é…ç½®ï¼‰

```typescript
// æ–°å¢æ–‡ä»¶ï¼šsrc/utils/LlmError.ts
export class LlmError extends Error {
  constructor(
    message: string,
    public code: string,
    public statusCode?: number,
  ) {
    super(message);
  }
}

export class ApiKeyError extends LlmError {}
export class NetworkError extends LlmError {}
export class StreamError extends LlmError {}
```

### ä¸­æœŸä¼˜åŒ–ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰

#### 4.3 æµå¼å“åº”æ ‡å‡†åŒ–
- ç»Ÿä¸€æ‰€æœ‰ Provider çš„æµå¼æ ¼å¼
- å­¦ä¹  Vercel AI SDK çš„ Stream ç±»

#### 4.4 Backend Proxy å…¼å®¹
- æ”¯æŒ Vercel AI SDK çš„è¯·æ±‚æ ¼å¼
- æä¾›åŒå‘å…¼å®¹

### é•¿æœŸè§„åˆ’ï¼ˆä½ä¼˜å…ˆçº§ï¼‰

#### 4.5 å·¥å…·è°ƒç”¨æ”¯æŒ
- å‚è€ƒ Vercel AI SDK çš„ Function Calling
- æ·»åŠ  tools å‚æ•°æ”¯æŒ

#### 4.6 å¤šæ¨¡æ€æ”¯æŒ
- å›¾ç‰‡è¾“å…¥è¾“å‡º
- éŸ³é¢‘æ”¯æŒ

---

## 5. ä¸å»ºè®®åšçš„äº‹

### âŒ å®Œå…¨é‡å†™
- ç°æœ‰æ¶æ„æ˜¯ä¼˜åŠ¿ï¼Œä¸æ˜¯åŠ£åŠ¿
- BYOK æ¨¡å¼æœ‰å¸‚åœºéœ€æ±‚
- æµè§ˆå™¨ AI æ˜¯å·®å¼‚åŒ–ç‰¹æ€§

### âŒ å¼ºè¡Œå…¼å®¹åç«¯æ¨¡å¼
- ä¼šç ´åå‰ç«¯ç›´è¿çš„ç®€æ´æ€§
- å¢åŠ ç”¨æˆ·é…ç½®è´Ÿæ‹…

### âŒ ç›²ç›®è¿½éš
- Vercel AI SDK é’ˆå¯¹ SaaS åœºæ™¯
- æˆ‘ä»¬çš„åœºæ™¯ä¸åŒ

---

## 6. å®æ–½è·¯çº¿å›¾

### Q1 2025ï¼ˆç«‹å³ï¼‰
- âœ… åˆ›å»º `useLlmChat` Hook
- âœ… ç»Ÿä¸€é”™è¯¯å¤„ç†
- âœ… ä¼˜åŒ–æ–‡æ¡£

### Q2 2025ï¼ˆ3ä¸ªæœˆï¼‰
- â³ Backend Proxy å¢å¼º
- â³ æµå¼å“åº”æ ‡å‡†åŒ–
- â³ æ·»åŠ æ›´å¤šæµ‹è¯•

### Q3 2025ï¼ˆ6ä¸ªæœˆï¼‰
- â³ å·¥å…·è°ƒç”¨æ”¯æŒ
- â³ æ’ä»¶ç³»ç»Ÿ
- â³ å¤šæ¨¡æ€æ¢ç´¢

---

## 7. æ€»ç»“

### æ ¸å¿ƒè§‚ç‚¹
1. **ä¸è¦å®Œå…¨é›†æˆ** - æ¶æ„å·®å¼‚å¤ªå¤§
2. **å€Ÿé‰´ä¼˜ç§€è®¾è®¡** - API è®¾è®¡ã€é”™è¯¯å¤„ç†
3. **ä¿æŒå·®å¼‚åŒ–** - BYOKã€æµè§ˆå™¨ AI æ˜¯ä¼˜åŠ¿
4. **æ¸è¿›å¼æ”¹è¿›** - ä¸ç ´åç°æœ‰åŠŸèƒ½

### æ¨èæ–¹æ¡ˆ
**æ–¹æ¡ˆ C + æ–¹æ¡ˆ D æ··åˆ**
- å­¦ä¹  Vercel AI SDK çš„ API è®¾è®¡
- ä¿æŒç°æœ‰æ¶æ„å’Œä¼˜åŠ¿
- æä¾›æ›´ç®€æ´çš„ Hook API
- Backend Proxy å¯é€‰å…¼å®¹

### ä¼˜å…ˆçº§
1. **P0** - åˆ›å»º `useLlmChat`ï¼ˆç®€åŒ– APIï¼‰
2. **P1** - ç»Ÿä¸€é”™è¯¯å¤„ç†
3. **P2** - Backend Proxy å¢å¼º
4. **P3** - å·¥å…·è°ƒç”¨æ”¯æŒ

---

## 8. ä»£ç ç¤ºä¾‹

### æ–° Hook ç¤ºä¾‹
```typescript
// src/hooks/useLlmChat.ts
import { useState, useCallback, FormEvent } from 'react';
import { useLlmConnector } from './useLlmConnector';
import { useChatManager } from './useChatManager';

export interface UseLlmChatConfig {
  clientName?: string;
  systemPrompt?: string;
  initialMessages?: ChatMessage[];
}

export const useLlmChat = (config: UseLlmChatConfig = {}) => {
  const { llmClient, states } = useLlmConnector(config.clientName);
  const { 
    messages, 
    sendMessage, 
    isStreaming, 
    error,
    clearMessages,
  } = useChatManager({
    clientName: config.clientName,
  });
  
  const [input, setInput] = useState('');
  
  const handleSubmit = useCallback((e?: FormEvent) => {
    e?.preventDefault();
    if (input.trim() && !isStreaming) {
      sendMessage(input);
      setInput('');
    }
  }, [input, isStreaming, sendMessage]);
  
  const handleInputChange = useCallback((e: React.ChangeEvent<HTMLInputElement>) => {
    setInput(e.target.value);
  }, []);
  
  return {
    // Messages
    messages,
    
    // Input
    input,
    setInput,
    handleInputChange,
    
    // Actions
    handleSubmit,
    reload: () => {}, // TODO
    stop: () => {}, // TODO
    append: sendMessage,
    
    // Status
    isLoading: isStreaming,
    error,
    
    // Utilities
    data: states,
  };
};
```

### ä½¿ç”¨ç¤ºä¾‹
```typescript
// ç®€å•ç”¨æ³• - ç±»ä¼¼ Vercel AI SDK
function ChatComponent() {
  const { messages, input, handleInputChange, handleSubmit, isLoading } = useLlmChat();
  
  return (
    <div>
      {messages.map((msg, i) => (
        <div key={i}>{msg.role}: {msg.content}</div>
      ))}
      
      <form onSubmit={handleSubmit}>
        <input value={input} onChange={handleInputChange} disabled={isLoading} />
        <button type="submit" disabled={isLoading}>
          {isLoading ? 'å‘é€ä¸­...' : 'å‘é€'}
        </button>
      </form>
    </div>
  );
}
```

---

**ç»“è®º**ï¼šå€Ÿé‰´ä¼˜ç§€è®¾è®¡ï¼Œä¿æŒæ ¸å¿ƒä¼˜åŠ¿ï¼Œæ¸è¿›å¼æ”¹è¿›ã€‚âœ…
