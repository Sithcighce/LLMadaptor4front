import express from 'express';
import cors from 'cors';
import dotenv from 'dotenv';
import fetch from 'node-fetch';
import { fileURLToPath } from 'url';
import { dirname, join } from 'path';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

// åŠ è½½çˆ¶ç›®å½•çš„ .env æ–‡ä»¶
dotenv.config({ path: join(__dirname, '..', '.env') });

const app = express();
const PORT = process.env.BACKEND_PORT || 3001;

// ä¸­é—´ä»¶
app.use(cors());
app.use(express.json());

// æ—¥å¿—ä¸­é—´ä»¶
app.use((req, res, next) => {
  console.log(`${new Date().toISOString()} - ${req.method} ${req.path}`);
  next();
});

// å¥åº·æ£€æŸ¥ç«¯ç‚¹
app.get('/api/ai/proxy/health', (req, res) => {
  res.json({
    status: 'ok',
    timestamp: new Date().toISOString(),
    availableProviders: ['openai', 'siliconflow', 'gemini', 'lmstudio']
  });
});

// ä¸»ä»£ç†ç«¯ç‚¹ - OpenAI æ ¼å¼
app.post('/api/ai/proxy', async (req, res) => {
  try {
    const { model, messages, stream = false, provider = 'siliconflow', ...otherParams } = req.body;

    console.log('ğŸ“¨ Proxy request:', {
      provider,
      model,
      messageCount: messages?.length,
      stream
    });

    // æ ¹æ® provider é€‰æ‹©å¯¹åº”çš„ API
    let apiUrl, apiKey, headers, requestBody;

    switch (provider) {
      case 'siliconflow':
        apiUrl = 'https://api.siliconflow.cn/v1/chat/completions';
        apiKey = process.env.SILICONFLOW_API_KEY;
        headers = {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apiKey}`
        };
        requestBody = {
          model: model || 'Qwen/Qwen2.5-7B-Instruct',
          messages,
          stream,
          ...otherParams
        };
        break;

      case 'openai':
        apiUrl = 'https://api.openai.com/v1/chat/completions';
        apiKey = process.env.OPENAI_API_KEY;
        headers = {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apiKey}`
        };
        requestBody = {
          model: model || 'gpt-3.5-turbo',
          messages,
          stream,
          ...otherParams
        };
        break;

      case 'gemini':
        const geminiModel = model || 'gemini-1.5-flash';
        apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/${geminiModel}:generateContent?key=${process.env.GEMINI_API_KEY}`;
        headers = {
          'Content-Type': 'application/json'
        };
        // Gemini ä½¿ç”¨ä¸åŒçš„æ ¼å¼ï¼Œéœ€è¦è½¬æ¢
        requestBody = {
          contents: messages.map(msg => ({
            role: msg.role === 'assistant' ? 'model' : 'user',
            parts: [{ text: msg.content }]
          }))
        };
        break;

      case 'lmstudio':
        apiUrl = 'http://127.0.0.1:1234/v1/chat/completions';
        headers = {
          'Content-Type': 'application/json'
        };
        requestBody = {
          model: model || 'local-model',
          messages,
          stream,
          ...otherParams
        };
        break;

      default:
        return res.status(400).json({
          error: `Unsupported provider: ${provider}`
        });
    }

    if (!apiKey && provider !== 'lmstudio') {
      return res.status(500).json({
        error: `API key not configured for provider: ${provider}`
      });
    }

    console.log('ğŸ”„ Forwarding to:', apiUrl);

    // å‘é€è¯·æ±‚
    const response = await fetch(apiUrl, {
      method: 'POST',
      headers,
      body: JSON.stringify(requestBody)
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error('âŒ API Error:', response.status, errorText);
      return res.status(response.status).json({
        error: `API Error: ${response.status} - ${errorText}`
      });
    }

    if (stream) {
      // æµå¼å“åº”
      res.setHeader('Content-Type', 'text/event-stream');
      res.setHeader('Cache-Control', 'no-cache');
      res.setHeader('Connection', 'keep-alive');

      // å¤„ç† Gemini çš„ç‰¹æ®Šæƒ…å†µï¼ˆGemini ä¸æ”¯æŒæµå¼ï¼‰
      if (provider === 'gemini') {
        const data = await response.json();
        const text = data.candidates?.[0]?.content?.parts?.[0]?.text || '';
        res.write(`data: ${JSON.stringify({
          choices: [{
            delta: { content: text },
            index: 0
          }]
        })}\n\n`);
        res.write('data: [DONE]\n\n');
        res.end();
      } else {
        // OpenAI å…¼å®¹æ ¼å¼çš„æµå¼å“åº”
        for await (const chunk of response.body) {
          res.write(chunk);
        }
        res.end();
      }
    } else {
      // éæµå¼å“åº”
      const data = await response.json();

      // å¤„ç† Gemini å“åº”æ ¼å¼è½¬æ¢
      if (provider === 'gemini') {
        const text = data.candidates?.[0]?.content?.parts?.[0]?.text || '';
        return res.json({
          choices: [{
            message: {
              role: 'assistant',
              content: text
            },
            finish_reason: 'stop',
            index: 0
          }],
          usage: {
            prompt_tokens: 0,
            completion_tokens: 0,
            total_tokens: 0
          },
          model: model || 'gemini-1.5-flash'
        });
      }

      // ç›´æ¥è¿”å› OpenAI æ ¼å¼çš„å“åº”
      res.json(data);
    }

    console.log('âœ… Request completed successfully');

  } catch (error) {
    console.error('âŒ Proxy error:', error);
    res.status(500).json({
      error: error.message || 'Internal server error'
    });
  }
});

// é”™è¯¯å¤„ç†
app.use((err, req, res, next) => {
  console.error('Server error:', err);
  res.status(500).json({
    error: 'Internal server error',
    message: err.message
  });
});

// å¯åŠ¨æœåŠ¡å™¨
app.listen(PORT, () => {
  console.log(`
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                            â•‘
â•‘   ğŸš€ LLM Backend Proxy Server                             â•‘
â•‘                                                            â•‘
â•‘   Status: Running                                          â•‘
â•‘   Port: ${PORT}                                           â•‘
â•‘   URL: http://localhost:${PORT}                           â•‘
â•‘                                                            â•‘
â•‘   Endpoints:                                               â•‘
â•‘   - POST /api/ai/proxy (main proxy)                       â•‘
â•‘   - GET  /api/ai/proxy/health (health check)             â•‘
â•‘                                                            â•‘
â•‘   Supported Providers:                                     â•‘
â•‘   âœ… SiliconFlow (ç¡…åŸºæµåŠ¨)                                â•‘
â•‘   âœ… OpenAI                                                â•‘
â•‘   âœ… Google Gemini                                         â•‘
â•‘   âœ… LM Studio (Local)                                     â•‘
â•‘                                                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  `);
  
  // æ£€æŸ¥ API Keys
  console.log('\nğŸ“‹ API Keys Status:');
  console.log(`   SiliconFlow: ${process.env.SILICONFLOW_API_KEY ? 'âœ… Configured' : 'âŒ Missing'}`);
  console.log(`   OpenAI:      ${process.env.OPENAI_API_KEY ? 'âœ… Configured' : 'âŒ Missing'}`);
  console.log(`   Gemini:      ${process.env.GEMINI_API_KEY ? 'âœ… Configured' : 'âŒ Missing'}`);
  console.log(`   LM Studio:   âœ… Local (http://127.0.0.1:1234)`);
  console.log('');
});
